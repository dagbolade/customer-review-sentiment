{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "04e57451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "49a3834a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>price</th>\n",
       "      <th>reviews</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey Dude</td>\n",
       "      <td>Wally Linen Natural</td>\n",
       "      <td>$64.99</td>\n",
       "      <td>Bought these for my son and he LOVES them!!! H...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey Dude</td>\n",
       "      <td>Wally Linen Natural</td>\n",
       "      <td>$64.99</td>\n",
       "      <td>I get lots of compliments for these .</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey Dude</td>\n",
       "      <td>Wally Linen Natural</td>\n",
       "      <td>$64.99</td>\n",
       "      <td>I love my Dude's!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey Dude</td>\n",
       "      <td>Wally Linen Natural</td>\n",
       "      <td>$64.99</td>\n",
       "      <td>Love these!!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey Dude</td>\n",
       "      <td>Wally Linen Natural</td>\n",
       "      <td>$64.99</td>\n",
       "      <td>Probably my favorite</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brand                model   price  \\\n",
       "0  Hey Dude  Wally Linen Natural  $64.99   \n",
       "1  Hey Dude  Wally Linen Natural  $64.99   \n",
       "2  Hey Dude  Wally Linen Natural  $64.99   \n",
       "3  Hey Dude  Wally Linen Natural  $64.99   \n",
       "4  Hey Dude  Wally Linen Natural  $64.99   \n",
       "\n",
       "                                             reviews  Ratings  \n",
       "0  Bought these for my son and he LOVES them!!! H...        5  \n",
       "1              I get lots of compliments for these .        5  \n",
       "2                                  I love my Dude's!        5  \n",
       "3                                       Love these!!        5  \n",
       "4                               Probably my favorite        5  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"./cleaned_zappos_men.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e4e5ca69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ratings\n",
       "5    2127\n",
       "4     254\n",
       "3     215\n",
       "1     184\n",
       "2     181\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Ratings\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "54ca7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove off by 1 error, ratings now start from 0 to 4\n",
    "if not df[\"Ratings\"].min() == 0:\n",
    "    df[\"Ratings\"] = df[\"Ratings\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "11cd1382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ratings\n",
       "4    2127\n",
       "3     254\n",
       "2     215\n",
       "0     184\n",
       "1     181\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Ratings\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "16ad566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWS = \"reviews\"\n",
    "LABELS = \"Ratings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5a6c8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_sample(df, target_column, strategy='median'):\n",
    "    \"\"\"\n",
    "    Perform hybrid sampling (oversample minority and downsample majority classes)\n",
    "    \n",
    "    Parameters:\n",
    "    @param: pd.DataFrame df: pandas DataFrame\n",
    "    @param: str target_column: column with class labels\n",
    "    @param: str or int strategy: 'median', 'mean', or integer for target size\n",
    "                or 'downsample_only', 'oversample_only'\n",
    "    @return: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Calculate class counts\n",
    "    class_counts = df[target_column].value_counts()\n",
    "    \n",
    "    # Determine target size based on strategy\n",
    "    if isinstance(strategy, int):\n",
    "        target_size = strategy\n",
    "    elif strategy == 'median':\n",
    "        target_size = int(class_counts.median())\n",
    "    elif strategy == 'mean':\n",
    "        target_size = int(class_counts.mean())\n",
    "    elif strategy == 'downsample_only':\n",
    "        target_size = class_counts.min()\n",
    "    elif strategy == 'oversample_only':\n",
    "        target_size = class_counts.max()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Use 'median', 'mean', integer, 'downsample_only', or 'oversample_only'\")\n",
    "    \n",
    "    # Group and sample\n",
    "    def sample_group(group):\n",
    "        n = len(group)\n",
    "        if n < target_size:  # Oversample minority classes\n",
    "            return group.sample(target_size, replace=True, random_state=42)\n",
    "        elif n > target_size:  # Downsample majority classes\n",
    "            return group.sample(target_size, random_state=42)\n",
    "        else:  # Keep as is\n",
    "            return group\n",
    "    \n",
    "    # Return a randomized transformed dataframe\n",
    "    return df.groupby(target_column, group_keys=False)[[col for col in df.columns]].apply(sample_group).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "84e0cdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>price</th>\n",
       "      <th>reviews</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clarks</td>\n",
       "      <td>Cotrell Edge</td>\n",
       "      <td>$69.99</td>\n",
       "      <td>These were uncomfortable right out of the box ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deer Stags</td>\n",
       "      <td>Status</td>\n",
       "      <td>$59.99</td>\n",
       "      <td>Doesn't fit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SKECHERS</td>\n",
       "      <td>Stamina AT Hands Free Slip-In Sneaker</td>\n",
       "      <td>$59.99</td>\n",
       "      <td>I like their look. The sides are stiffer than ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fila</td>\n",
       "      <td>Disruptor II Premium</td>\n",
       "      <td>$46.52</td>\n",
       "      <td>Order half size up.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>etnies</td>\n",
       "      <td>Kingpin</td>\n",
       "      <td>$69.95</td>\n",
       "      <td>They Look Great but They Definitely didn't Fee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        brand                                  model   price  \\\n",
       "0      Clarks                           Cotrell Edge  $69.99   \n",
       "1  Deer Stags                                 Status  $59.99   \n",
       "2    SKECHERS  Stamina AT Hands Free Slip-In Sneaker  $59.99   \n",
       "3        Fila                   Disruptor II Premium  $46.52   \n",
       "4      etnies                                Kingpin  $69.95   \n",
       "\n",
       "                                             reviews  Ratings  \n",
       "0  These were uncomfortable right out of the box ...        1  \n",
       "1                                        Doesn't fit        1  \n",
       "2  I like their look. The sides are stiffer than ...        2  \n",
       "3                                Order half size up.        4  \n",
       "4  They Look Great but They Definitely didn't Fee...        1  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df = hybrid_sample(df, target_column=\"Ratings\", strategy=\"oversample_only\")\n",
    "oversampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "ee23dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building BPE (byte pair encoding) Tokenizer\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size: int = 50, stop_word: str = \"</w>\"):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}  # Stores merge operations\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Vocabulary with special tokens\n",
    "        self.stop_word = stop_word\n",
    "        \n",
    "    def _get_pairs(self, word_list: list):\n",
    "        \"\"\"Gets the frequency pair from a wordlist\"\"\"\n",
    "        pairs = Counter()\n",
    "        for word in word_list:\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab(self, word_list: list, pair: tuple):\n",
    "        \"\"\"Merges the most frequent pair in the vocabulary\"\"\"\n",
    "        new_word_list = []\n",
    "        for word in word_list:\n",
    "            i = 0\n",
    "            new_word = []\n",
    "            while i < len(word.split()):\n",
    "                if (i < len(word.split())-1 and \n",
    "                    word.split()[i] == pair[0] and \n",
    "                    word.split()[i+1] == pair[1]):\n",
    "                    new_word.append(pair[0]+pair[1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word.split()[i])\n",
    "                    i += 1\n",
    "            new_word_list.append(\" \".join(new_word))\n",
    "        return new_word_list\n",
    "    \n",
    "    def train(self, corpus: str):\n",
    "        \"\"\"Train BPE on a given corpus.\"\"\"\n",
    "        # Preprocess the corpus\n",
    "        words = corpus.split()\n",
    "        word_list = [\" \".join(word) + \" \" + self.stop_word for word in words]\n",
    "\n",
    "        # Initialize vocabulary with characters\n",
    "        for word in word_list:\n",
    "            for char in word.split():\n",
    "                if char not in self.vocab:\n",
    "                    self.vocab[char] = len(self.vocab)\n",
    "\n",
    "        # Perform merges until we reach the desired vocab size\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = self._get_pairs(word_list)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Add the new merged token to vocabulary\n",
    "            if best_pair[0] + best_pair[1] not in self.vocab:\n",
    "                self.vocab[best_pair[0] + best_pair[1]] = len(self.vocab)\n",
    "            \n",
    "            word_list = self._merge_vocab(word_list, best_pair)\n",
    "\n",
    "        print(\"Final Vocabulary:\", self.vocab)\n",
    "        \n",
    "    def encode(self, text: str, max_length: int = None):\n",
    "        \"\"\"Encodes text into subword tokens and returns the tensor indices\"\"\"\n",
    "        # Split into words first\n",
    "        words = text.split()\n",
    "        all_tokens = []\n",
    "        all_token_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Initialize with characters for each word\n",
    "            word_processed = \" \".join(word) + \" \" + self.stop_word\n",
    "            \n",
    "            # Apply all possible merges\n",
    "            while True:\n",
    "                pairs = self._get_pairs([word_processed])\n",
    "                if not pairs:\n",
    "                    break\n",
    "                # Find the merge that appears first in our merge list\n",
    "                best_pair = None\n",
    "                for pair in self.merges:\n",
    "                    if pair in pairs:\n",
    "                        best_pair = pair\n",
    "                        break\n",
    "                if not best_pair:\n",
    "                    break\n",
    "                word_processed = self._merge_vocab([word_processed], best_pair)[0]\n",
    "            \n",
    "            tokens = word_processed.split()\n",
    "            token_ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
    "            \n",
    "            all_tokens.extend(tokens)\n",
    "            all_token_ids.extend(token_ids)\n",
    "        \n",
    "        # Add padding if max_length is specified\n",
    "        if max_length is not None:\n",
    "            if len(all_token_ids) < max_length:\n",
    "                all_token_ids += [self.vocab[\"<PAD>\"]] * (max_length - len(all_token_ids))\n",
    "            elif len(all_token_ids) > max_length:\n",
    "                all_token_ids = all_token_ids[:max_length]\n",
    "                \n",
    "        return all_tokens, all_token_ids\n",
    "        \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode indices back to text.\"\"\"\n",
    "        tokens = []\n",
    "        for idx in token_ids:\n",
    "            for token, token_id in self.vocab.items():\n",
    "                if token_id == idx:\n",
    "                    tokens.append(token)\n",
    "                    break\n",
    "        \n",
    "        # Reconstruct words by splitting at stop_word tokens\n",
    "        decoded_text = []\n",
    "        current_word = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.endswith(self.stop_word):\n",
    "                current_word.append(token.replace(self.stop_word, \"\"))\n",
    "                decoded_text.append(\"\".join(current_word))\n",
    "                current_word = []\n",
    "            else:\n",
    "                current_word.append(token)\n",
    "        \n",
    "        # Handle any remaining tokens (if no stop_word at end)\n",
    "        if current_word:\n",
    "            decoded_text.append(\"\".join(current_word))\n",
    "        \n",
    "        return \" \".join(decoded_text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save the tokenizer to a JSON file\"\"\"\n",
    "        import json\n",
    "        save_data = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'stop_word': self.stop_word,\n",
    "            'merges': {f\"{k[0]},{k[1]}\": v for k, v in self.merges.items()},\n",
    "            'vocab': self.vocab\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(save_data, f, indent=2)\n",
    "\n",
    "        print(\"Succesfully saved tokenzer\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        \"\"\"Load a tokenizer from a JSON file\"\"\"\n",
    "        import json\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        tokenizer = cls(vocab_size=data['vocab_size'], stop_word=data['stop_word'])\n",
    "\n",
    "        # Convert string keys back to tuples\n",
    "        tokenizer.merges = {\n",
    "            tuple(k.split(',')): v for k, v in data['merges'].items()\n",
    "        }\n",
    "\n",
    "        tokenizer.vocab = data['vocab']\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a19bb9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary: {'<PAD>': 0, '<UNK>': 1, 'T': 2, 'o': 3, 'k': 4, 'e': 5, 'n': 6, 'i': 7, 'z': 8, 'a': 9, 't': 10, '</w>': 11, 's': 12, 'h': 13, 'p': 14, 'r': 15, 'c': 16, 'f': 17, 'b': 18, 'g': 19, 'd': 20, 'w': 21, 'x': 22, 'm': 23, 'l': 24, 'u': 25, 's</w>': 26, 'ni': 27, 'n</w>': 28, 'in': 29, 'To': 30, 'Tok': 31, 'Toke': 32, 'Tokeni': 33, 'Tokeniz': 34, 'Tokeniza': 35, 'Tokenizat': 36, 'Tokenizati': 37, 'Tokenizatio': 38, 'Tokenization</w>': 39, 'is</w>': 40, 'th': 41, 'the': 42, 'the</w>': 43, 'pr': 44, 'pro': 45, 'proc': 46, 'proce': 47, 'proces': 48, 'process</w>': 49, 'of': 50, 'of</w>': 51, 'br': 52, 'bre': 53, 'brea': 54, 'break': 55, 'breakin': 56, 'breaking': 57, 'breaking</w>': 58, 'do': 59, 'dow': 60, 'down</w>': 61, 'te': 62, 'tex': 63, 'text': 64, 'text</w>': 65, 'int': 66, 'into': 67, 'into</w>': 68, 'sm': 69, 'sma': 70, 'smal': 71, 'small': 72, 'smalle': 73, 'smaller': 74, 'smaller</w>': 75, 'uni': 76, 'unit': 77, 'units</w>': 78}\n",
      "Encoded Tokens: ['breaking</w>', 'down</w>', 'the</w>', 'text</w>']\n",
      "Token IDs: [58, 61, 43, 65]\n",
      "Decoded Text: breaking down the text\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer usage\n",
    "corpus = \"Tokenization is the process of breaking down text into smaller units\"\n",
    "test_tokenizer = BPETokenizer(vocab_size=100)\n",
    "test_tokenizer.train(corpus)\n",
    "\n",
    "# Encoding example\n",
    "tokens, token_ids = test_tokenizer.encode(\"breaking down the text\")\n",
    "print(\"Encoded Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Decoding example\n",
    "decoded_text = test_tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "52e59d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bought these for my son and he LOVES them!!! He wears them almost every day! I will definitely be bu'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = \"\\n\".join(df[REVIEWS].to_list())\n",
    "CORPUS[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c57703e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary: {'<PAD>': 0, '<UNK>': 1, 'B': 2, 'o': 3, 'u': 4, 'g': 5, 'h': 6, 't': 7, '</w>': 8, 'e': 9, 's': 10, 'f': 11, 'r': 12, 'm': 13, 'y': 14, 'n': 15, 'a': 16, 'd': 17, 'L': 18, 'O': 19, 'V': 20, 'E': 21, 'S': 22, '!': 23, 'H': 24, 'w': 25, 'l': 26, 'v': 27, 'I': 28, 'i': 29, 'b': 30, 'p': 31, 'C': 32, '.': 33, 'c': 34, 'D': 35, \"'\": 36, 'P': 37, 'A': 38, 'z': 39, 'T': 40, 'k': 41, 'W': 42, ',': 43, '1': 44, 'j': 45, 'q': 46, '…': 47, '-': 48, '&': 49, 'Z': 50, '’': 51, 'J': 52, 'x': 53, 'G': 54, '5': 55, '0': 56, '(': 57, ')': 58, ':': 59, 'F': 60, '8': 61, '+': 62, '\"': 63, 'M': 64, 'N': 65, 'R': 66, '9': 67, '‘': 68, '/': 69, '2': 70, '?': 71, ';': 72, '“': 73, '”': 74, '3': 75, '4': 76, 'Y': 77, '6': 78, 'Â': 79, '½': 80, 'U': 81, 'K': 82, '=': 83, '\\\\': 84, '7': 85, 'á': 86, '$': 87, 'X': 88, '%': 89, '#': 90, '*': 91, 'Q': 92, '~': 93, ']': 94, '—': 95, '>': 96, '¹': 97, '[': 98, '`': 99, '–': 100, 'e</w>': 101, 'th': 102, 't</w>': 103, 's</w>': 104, 'd</w>': 105, '.</w>': 106, 'y</w>': 107, 'er': 108, 'an': 109, 'in': 110, 'or': 111, 'ar': 112, 'o</w>': 113, 'the</w>': 114, 'and</w>': 115, 'I</w>': 116, 'the': 117, 'sh': 118, 'on': 119, 'al': 120, ',</w>': 121, 'ou': 122, 're': 123, 'for': 124, 'a</w>': 125, 'sho': 126, 'om': 127, 'ing': 128, 'en': 129, 'er</w>': 130, 'to</w>': 131, 'ing</w>': 132, 'es</w>': 133, 'is</w>': 134, 'ed</w>': 135, 'f</w>': 136, 'se</w>': 137, 'Th': 138, 'ha': 139, 'oo': 140, 've</w>': 141, 'wi': 142, 'st': 143, 'li': 144, 'ab': 145, 'l</w>': 146, 'le</w>': 147, 'ly</w>': 148, 'com': 149, 'at</w>': 150, 'si': 151, 'm</w>': 152, 'e.</w>': 153, 'we': 154, 'of</w>': 155, 'for</w>': 156, 'gh': 157, 'in</w>': 158, 'fe': 159, 'are</w>': 160, 'as</w>': 161, 'ti': 162, 'bu': 163, 'it</w>': 164, 'k</w>': 165, 'es': 166, 'on</w>': 167, 'fort': 168, 'ver': 169, 'it': 170, 'my</w>': 171, 'ch': 172, 'th</w>': 173, 'su': 174, 'shoes</w>': 175, 'they</w>': 176, 'ir': 177, 'fortab': 178, 'at': 179, 'ow': 180, 'ea': 181, 'comfortab': 182, 'these</w>': 183, 'st</w>': 184, 'ac': 185, 'but</w>': 186, 'an</w>': 187, 'The': 188, 'have</w>': 189, '!</w>': 190, 'siz': 191, 'ic': 192, 'pp': 193, 'ot</w>': 194, 'wear': 195, 'ol': 196, 'pa': 197, 'with</w>': 198, 'them</w>': 199, 'loo': 200, 'be': 201, 'en</w>': 202, 'll</w>': 203, 'shoe</w>': 204, 'fee': 205, 'to': 206, 'n</w>': 207, 'ur': 208, 'that</w>': 209, 'so</w>': 210, 'The</w>': 211, 'e,</w>': 212, 'p</w>': 213, 'wh': 214, 'ke</w>': 215, 's.</w>': 216, 'ght</w>': 217, 'di': 218, 'was</w>': 219, 'very</w>': 220, 'gre': 221, 'pair': 222, 'ri': 223, 'comfortable</w>': 224, 't.</w>': 225, 'lo': 226, 'size</w>': 227, 'ally</w>': 228, 'you': 229, 'al</w>': 230, 'de</w>': 231, 'not</w>': 232, 'or</w>': 233, 'ch</w>': 234, 'tt': 235, 'se': 236, 'ne': 237, 'oul': 238, 'ere</w>': 239, 'out</w>': 240, 'wor': 241, 'ro': 242, 'this</w>': 243, 'me</w>': 244, 'ca': 245, 'wear</w>': 246, 'da': 247, 'ma': 248, 'be</w>': 249, 'fit</w>': 250, 'like</w>': 251, 'ther</w>': 252, 'ow</w>': 253, 'ex': 254, 'ru': 255, 'shoes': 256, 'so': 257, 'per': 258, 'de': 259, \"'t</w>\": 260, 'They</w>': 261, 'ould</w>': 262, \"I'\": 263, 'le': 264, 'king</w>': 265, 'sty': 266, 'd.</w>': 267, 'all</w>': 268, 'pair</w>': 269, 'y.</w>': 270, 'sa': 271, 'qu': 272, 'lac': 273, 'la': 274, 'These</w>': 275, 'men': 276, 'great</w>': 277, 'mo': 278, 'bo': 279, 'le.</w>': 280, \"'s</w>\": 281, 'el': 282, 'sn': 283, 'un': 284, 'fec': 285, 'ge': 286, 'you</w>': 287, 'ood</w>': 288, 'der': 289, 'ks</w>': 290, 'ust</w>': 291, 'ore</w>': 292, 'wal': 293, 'too</w>': 294, 'wa': 295, 'than</w>': 296, 'om</w>': 297, 'look</w>': 298, 'ong': 299, 'were</w>': 300, 'ff': 301, 'them': 302, 'tr': 303, 'con': 304, 'supp': 305, 'hi': 306, 'w</w>': 307, 'gh</w>': 308, 'eak': 309, 'use</w>': 310, 'pe': 311, 'foo': 312, 'fr': 313, 'ear': 314, 'ely</w>': 315, 'all': 316, 'ver</w>': 317, 'ent</w>': 318, 'cu': 319, 'suppor': 320, 'rea': 321, 'other</w>': 322, 's,</w>': 323, 'had</w>': 324, 'fin': 325, 'col': 326, 'sli': 327, 'perfec': 328, 'just</w>': 329, 'sed</w>': 330, 'more</w>': 331, 'shoes.</w>': 332, 'feet</w>': 333, 'would</w>': 334, 'vi': 335, 'one</w>': 336, 'ought</w>': 337, 'love</w>': 338, 'sneak': 339, 'ice</w>': 340, 're</w>': 341, 'oun': 342, '..': 343, 'ers</w>': 344, 'ery</w>': 345, 'wid': 346, 'good</w>': 347, 'lea': 348, 'som': 349, 't,</w>': 350, 'pro': 351, 'ke': 352, 'ter</w>': 353, 'sol': 354, 'wide</w>': 355, 'shoe.</w>': 356, 'lar': 357, 'up</w>': 358, 'Very</w>': 359, 'get</w>': 360, 'ce</w>': 361, '5</w>': 362, 'Gre': 363, 'from</w>': 364, 'tur': 365, 'y,</w>': 366, 'Great</w>': 367, 'ing.</w>': 368, 'fit': 369, 'day</w>': 370, 'le,</w>': 371, 'order': 372, 'arr': 373, 'sm': 374, 'will</w>': 375, 'ag': 376, 'whi': 377, 'comfortable.</w>': 378, 'alit': 379, \"I'm</w>\": 380, 'ho': 381, '2</w>': 382, 'ts</w>': 383, 'them.</w>': 384, 'ig': 385, 'ys</w>': 386, 'most</w>': 387, 'My</w>': 388, 'er.</w>': 389, 'us': 390, 'really</w>': 391, 'can</w>': 392, 'cause</w>': 393, 'narr': 394, 'year': 395, 'qualit': 396, 'ough</w>': 397, 'This</w>': 398, 'ds</w>': 399, 'r</w>': 400, 'pre': 401, 'great': 402, 'ted</w>': 403, 'feel</w>': 404, 'mi': 405, 'sti': 406, 'am</w>': 407, 'comp': 408, 'ut</w>': 409, 'cha': 410, 'has</w>': 411, 'pur': 412, 'he': 413, 'looking</w>': 414, 'sh</w>': 415, 'long': 416, 'fter</w>': 417, 'ta': 418, 'when</w>': 419, '10': 420, 'if</w>': 421, 'do': 422, 'fir': 423, 'litt': 424, 'about</w>': 425, 'mu': 426, 'because</w>': 427, 'ue</w>': 428, 'per</w>': 429, 'own</w>': 430, 'app': 431, 'don': 432, 'ite</w>': 433, 'pr': 434, 'ound</w>': 435, 'little</w>': 436, 'sole</w>': 437, 'bought</w>': 438, 'll': 439, 'purcha': 440, 'tion': 441, 'd,</w>': 442, 'te': 443, 'es.</w>': 444, \"I've</w>\": 445, 'ordered</w>': 446, 'tly</w>': 447, 'no</w>': 448, 'sual</w>': 449, 'big': 450, 'br': 451, 'wearing</w>': 452, 'commen': 453, 'been</w>': 454, 'fu': 455, 'pec': 456, 'me': 457, 'der</w>': 458, 'fa': 459, ')</w>': 460, 'ded</w>': 461, 'retur': 462, 'style</w>': 463, 'any</w>': 464, '.5</w>': 465, 'Lo': 466, 'foot</w>': 467, 'able</w>': 468, 'which</w>': 469, '-</w>': 470, 'box': 471, 'tic': 472, 'ev': 473, \"'re</w>\": 474, 'dn': 475, 'well</w>': 476, 'ater': 477, 'some</w>': 478, 'tion</w>': 479, 'thing</w>': 480, 'do</w>': 481, 'he</w>': 482, 'po': 483, 'mon': 484, 'ever': 485, 'bac': 486, 'lt</w>': 487, 'support</w>': 488, 'arch</w>': 489, 'shi': 490, 'first</w>': 491, 'ra': 492, 'rec': 493, 'go': 494, 'k.</w>': 495, 'ding</w>': 496, 'recommen': 497, 'ked</w>': 498, 'ess</w>': 499, 'now</w>': 500, 'after</w>': 501, 'ved</w>': 502, 'op': 503, 'diff': 504, 'sy</w>': 505, 'ep': 506, 'tru': 507, 'casual</w>': 508, 'larg': 509, 'color': 510, 'bit</w>': 511, 'narrow</w>': 512, 'quality</w>': 513, 'se.</w>': 514, 'only</w>': 515, 'ate</w>': 516, 'toe</w>': 517, 'your</w>': 518, 'right</w>': 519, 'time</w>': 520, 'Love</w>': 521, 'slip</w>': 522, 'got</w>': 523, 'brea': 524, \"don't</w>\": 525, 'hal': 526, 'ff</w>': 527, 'go</w>': 528, 'He</w>': 529, 'styli': 530, 'It</w>': 531, 'par': 532, 'no': 533, 'el</w>': 534, 'wha': 535, 'every': 536, 'bett': 537, 'orm': 538, 'ad': 539, 'buy</w>': 540, 'much</w>': 541, '11': 542, 'still</w>': 543, 'off': 544, 'soc': 545, 'sk': 546, \"y're</w>\": 547, 'small': 548, 'work</w>': 549, 'worn</w>': 550, 'uc': 551, 'way</w>': 552, 'usb': 553, 'est</w>': 554, 'walking</w>': 555, 'I’': 556, '!!</w>': 557, 'long</w>': 558, 'half</w>': 559, 'ar</w>': 560, 'perfect</w>': 561, '’t</w>': 562, 'size.</w>': 563, 'nice</w>': 564, 'norm': 565, 'ard</w>': 566, 'even</w>': 567, 'ans</w>': 568, 'me.</w>': 569, 'feel': 570, 'back</w>': 571, 'ic</w>': 572, 'true</w>': 573, 'differ': 574, 'gi': 575, 'ei': 576, 'ateri': 577, 'wan': 578, 'comfortable,</w>': 579, 'husb': 580, 'color</w>': 581, 'see': 582, 'what</w>': 583, 'blac': 584, '\"</w>': 585, 'put</w>': 586, \"dn't</w>\": 587, 'sig': 588, 'also</w>': 589, '’s</w>': 590, 'leather</w>': 591, 'ying</w>': 592, 'great.</w>': 593, 'around</w>': 594, 'kes</w>': 595, 'Com': 596, 'another</w>': 597, 'tri': 598, 'fort</w>': 599, 'old</w>': 600, 'materi': 601, 'appo': 602, 'happ': 603, 'im': 604, 'can': 605, 'in.</w>': 606, 'with': 607, 'ell': 608, 'tu': 609, 'how</w>': 610, 'and': 611, 'ther': 612, 'Zappo': 613, 'ves</w>': 614, 'super</w>': 615, 'felt</w>': 616, 'lu': 617, 'ter': 618, 'well': 619, 'side</w>': 620, 'new</w>': 621, 'ft</w>': 622, '3</w>': 623, 'ned</w>': 624, 'exc': 625, 'shoes,</w>': 626, 'on.</w>': 627, 'too': 628, 'ssi': 629, 'ss</w>': 630, 'te</w>': 631, 'ever</w>': 632, 'wee': 633, 'soles</w>': 634, 'sneakers</w>': 635, 'his</w>': 636, 'ang': 637, 'sin': 638, 'feet.</w>': 639, 'ser': 640, 'shoe,</w>': 641, 'easy</w>': 642, 'is': 643, 'pairs</w>': 644, 'ght': 645, 'better</w>': 646, 'recommend</w>': 647, 'any': 648, 'made</w>': 649, 'different</w>': 650, 'comfort</w>': 651, 'same</w>': 652, 'gu': 653, 'i</w>': 654, 'aga': 655, 'husband</w>': 656, 'box</w>': 657, 'prob': 658, 'month': 659, 'wom': 660, 'expec': 661, 'ir</w>': 662, 'tre': 663, 'walk</w>': 664, 'vie': 665, 'E</w>': 666, 'il': 667, 'tho': 668, 'desig': 669, 'ways</w>': 670, 'width</w>': 671, 'run</w>': 672, 'hou': 673, 'laces</w>': 674, 'nee': 675, 'up': 676, 'by</w>': 677, 'dy</w>': 678, 'order</w>': 679, \"it's</w>\": 680, 'last</w>': 681, 'ty': 682, 'So</w>': 683, 'S</w>': 684, 'black</w>': 685, 'need</w>': 686, 'n.</w>': 687, 'ba': 688, 'out': 689, 'could</w>': 690, 'some': 691, 'pic': 692, 'high</w>': 693, 'revie': 694, 'wei': 695, 'sp': 696, 'end</w>': 697, 'down</w>': 698, 'light</w>': 699, 'find</w>': 700, 'ep</w>': 701, 'there</w>': 702, 'looks</w>': 703, 'Re': 704, 'over</w>': 705, 'heel</w>': 706, 'Ske': 707, 'dre': 708, 'ving</w>': 709, 'If</w>': 710, 'Wi': 711, 'tl': 712, 'star': 713, ').</w>': 714, 'make</w>': 715, 'tong': 716, 'cushi': 717, 'goo': 718, 'wore</w>': 719, 'enough</w>': 720, 'son</w>': 721, 'es,</w>': 722, '1/': 723, 'off</w>': 724, 'purchased</w>': 725, 'ank': 726, 'wo</w>': 727, 'thin': 728, 'plac': 729, 'without</w>': 730, 'return</w>': 731, 'ough': 732, 'Su': 733, 'run': 734, 'ing,</w>': 735, 'did</w>': 736, 'always</w>': 737, 'Al': 738, 'Sho': 739, 'sur': 740, 'finit': 741, 'finitely</w>': 742, '...</w>': 743, 'walk': 744, 'few</w>': 745, 'years</w>': 746, 'support.</w>': 747, 'shoe': 748, 'fl': 749, 'ning</w>': 750, 'An': 751, 'maz': 752, 'ed.</w>': 753, 'Good</w>': 754, 'Not</w>': 755, 'white</w>': 756, 'Ha': 757, 'big</w>': 758, 'thr': 759, 'fit.</w>': 760, 'try</w>': 761, '0</w>': 762, 'every</w>': 763, 'ous</w>': 764, 'sneaker': 765, 'dur': 766, 'A</w>': 767, 'less</w>': 768, 'But</w>': 769, 'last': 770, 'tting</w>': 771, 'loves</w>': 772, 'tty</w>': 773, 'stylish</w>': 774, 'I’m</w>': 775, 'going</w>': 776, 'happy</w>': 777, 'into</w>': 778, '10</w>': 779, 'Comfortab': 780, 'their</w>': 781, 'sneaker</w>': 782, 'two</w>': 783, 'chers</w>': 784, 'oc': 785, '.5': 786, 'comf': 787, 'min': 788, 'never</w>': 789, 'ni': 790, 'id</w>': 791, 'ance</w>': 792, 'narrow': 793, 'Ho': 794, 'ye': 795, 'wear.</w>': 796, 'issu': 797, 'feels</w>': 798, 'ting</w>': 799, 'ppo': 800, 'tive</w>': 801, 'k,</w>': 802, 'over': 803, 'ool</w>': 804, 'pretty</w>': 805, 'Zappos</w>': 806, 'ty</w>': 807, 'proble': 808, 'tongue</w>': 809, 'lar</w>': 810, 'ts.</w>': 811, 'mely</w>': 812, 'day.</w>': 813, 'where</w>': 814, 'ten': 815, 'these.</w>': 816, \"they're</w>\": 817, 'ms</w>': 818, 'review': 819, 'hard</w>': 820, 'it.</w>': 821, '...': 822, 'being</w>': 823, 'recei': 824, 'cond</w>': 825, 'ght.</w>': 826, 'hel': 827, 'that': 828, 'who</w>': 829, 'months</w>': 830, 'pe</w>': 831, 'women': 832, 'sappo': 833, 'sappoin': 834, 'used</w>': 835, 'tremely</w>': 836, 'lin': 837, 'socks</w>': 838, 'ci': 839, 'les</w>': 840, 's!</w>': 841, 'And</w>': 842, 'width': 843, 'sta': 844, '1/2</w>': 845, 'duc': 846, 'well.</w>': 847, 'av': 848, 'rub': 849, 'ut': 850, 'mm': 851, 'pu': 852, 'come</w>': 853, 'top</w>': 854, 'foot.</w>': 855, 'peri': 856, 'It': 857, 'comfort': 858, 'T</w>': 859, 'ble</w>': 860, 'see</w>': 861, 'pri': 862, '8</w>': 863, 'amaz': 864, \"didn't</w>\": 865, 'cla': 866, 'perfectl': 867, 'think</w>': 868, 'since</w>': 869, 'many</w>': 870, 'm.</w>': 871, 'both</w>': 872, 'medi': 873, 'fits</w>': 874, 'best</w>': 875, 'ort</w>': 876, 'fi': 877, 'clo': 878, 'ick': 879, 'tight</w>': 880, 'tried</w>': 881, 'Ne': 882, 'wider</w>': 883, 'ably</w>': 884, \"They're</w>\": 885, 'disappoin': 886, 'ately</w>': 887, 'fac': 888, 'pl': 889, 'ven': 890, 'befor': 891, 'sually</w>': 892, 'ple</w>': 893, 'on,</w>': 894, 'sure</w>': 895, 'almost</w>': 896, 'Li': 897, 'days</w>': 898, 'far</w>': 899, 'larger</w>': 900, 'second</w>': 901, 'tic</w>': 902, 'Per': 903, 'cre': 904, 'war': 905, 'cust': 906, '7</w>': 907, 'ug': 908, '4</w>': 909, 'peci': 910, '&</w>': 911, '9</w>': 912, 'On': 913, 'stylish': 914, 'normally</w>': 915, 'esn': 916, 'ellent</w>': 917, 'found</w>': 918, 'chang': 919, 'stan': 920, 'ole</w>': 921, 'fortun': 922, '12</w>': 923, 'small</w>': 924, 'fect</w>': 925, 'material</w>': 926, 'pair.</w>': 927, 'replac': 928, 'while</w>': 929, 'then</w>': 930, 'came</w>': 931, 'break</w>': 932, 'c</w>': 933, 'tim': 934, 'bran': 935, 'everyday</w>': 936, 'ily</w>': 937, 'actu': 938, 'red</w>': 939, 'definitely</w>': 940, 'compli': 941, 'complimen': 942, '11</w>': 943, 'ful</w>': 944, 'jo': 945, 'versi': 946, '6</w>': 947, 'pper</w>': 948, 'him</w>': 949, 'hea': 950, 'shoes!</w>': 951, 'Fit</w>': 952, 'material': 953, ';</w>': 954, 'its</w>': 955, 'foot': 956, 'lot</w>': 957, 'wanted</w>': 958, 'ps</w>': 959, 'buying</w>': 960, 'Nice</w>': 961, 'ice.</w>': 962, 'um': 963, 'des</w>': 964, 'say</w>': 965, 'Ex': 966, 'ghly</w>': 967, 'able.</w>': 968, 'doesn': 969, 'Vans</w>': 970, 'cou': 971, 'through</w>': 972, 'Un': 973, 'se,</w>': 974, 'we</w>': 975, 'provi': 976, 'ther.</w>': 977, 'keep</w>': 978, '?</w>': 979, 'ght,</w>': 980, 'D</w>': 981, 'p-': 982, 'should</w>': 983, 'dow': 984, 'vor': 985, 'je': 986, 'ready</w>': 987, 'extremely</w>': 988, 'Super</w>': 989, 'pad': 990, 'ks.</w>': 991, 'seem</w>': 992, 'easi': 993, 'er,</w>': 994, 'light': 995, 'won': 996, 'Comfortable</w>': 997, 'stru': 998, 've': 999, 'design</w>': 1000, 'ber</w>': 1001, 'mar': 1002, 'want</w>': 1003, 'know</w>': 1004, 'cushion': 1005, 'ortho': 1006, 'Pro': 1007, 'weight</w>': 1008, 'though</w>': 1009, 'gr': 1010, 'normal</w>': 1011, 'produc': 1012, 'now': 1013, 'usually</w>': 1014, 'tion.</w>': 1015, 'smaller</w>': 1016, 'As</w>': 1017, 'Perfect</w>': 1018, 'tin': 1019, 'before</w>': 1020, 'Y</w>': 1021, 'dis': 1022, \"ouldn't</w>\": 1023}\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "# Train the tokenizer\n",
    "tokenizer = BPETokenizer(vocab_size=1024)\n",
    "tokenizer.train(CORPUS)\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "32d9c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE: 2369\n",
      "TEST SIZE: 592\n"
     ]
    }
   ],
   "source": [
    "# Create a Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X: list, y: list, tokenizer: BPETokenizer, max_length: int=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get current text and label\n",
    "        text = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        tokens, token_ids = self.tokenizer.encode(text, self.max_length)\n",
    "        \n",
    "        if len(tokens) > self.max_length:\n",
    "            print(f\"WARNING: Token greater than max length, truncating\")\n",
    "        \n",
    "        # Convert to tensor immediately\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return token_ids, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # Separate inputs and labels\n",
    "        input_ids = [item[0] for item in batch]\n",
    "        labels = [item[1] for item in batch]\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_inputs = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # Stack labels (now using list instead of generator)\n",
    "        labels = torch.stack(labels)\n",
    "        \n",
    "        return padded_inputs, labels\n",
    "    \n",
    "# Variables setup\n",
    "X = df[REVIEWS].to_list()\n",
    "y = df[LABELS].to_list()\n",
    "MAX_LENGTH = 500 # Maximum sequence length\n",
    "test_size = int(len(y) * 0.2)\n",
    "train_size = int(len(y) - test_size)\n",
    "\n",
    "# Initialize the dataset\n",
    "full_dataset = TextDataset(X=X, y=y, tokenizer=tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "# Generator for spliting dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "# Train test split\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size], generator=generator)\n",
    "print(f\"TRAIN SIZE: {len(train_dataset)}\\nTEST SIZE: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "f8fc8eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([367, 641, 699, 115, 220, 224,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " tensor(4))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a4df6f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great shoe, light and very comfortable <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[1000][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c468470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders for training\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=TextDataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=TextDataset.collate_fn)# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7722e05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 561,  250,    0,  ...,    0,    0,    0],\n",
       "         [ 359,  224,  115,  ...,    0,    0,    0],\n",
       "         [ 116,  407,  220,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 380,  408,  112,  ...,    0,    0,    0],\n",
       "         [ 380, 1014,  187,  ...,    0,    0,    0],\n",
       "         [1007,   30,  884,  ...,    0,    0,    0]]),\n",
       " tensor([4, 4, 4, 3, 4, 4, 4, 3, 4, 1, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4,\n",
       "         4, 4, 4, 4, 4, 1, 2, 4]))"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloader test\n",
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "67560126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Multiple convolutional layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, \n",
    "                      out_channels=num_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len]\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = [batch size, seq len, emb dim]\n",
    "        \n",
    "        # Conv1d expects [batch size, channels, seq len]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        # embedded = [batch size, emb dim, seq len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conved_n = [batch size, num_filters, seq len - filter_size + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled_n = [batch size, num_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch size, num_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.fc(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c6ccd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = MAX_LENGTH\n",
    "NUM_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = len(set(y))  # Number of classes\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "\n",
    "# Initialize model\n",
    "model = TextCNN(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "873ca648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(1024, 500)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(500, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(500, 100, kernel_size=(4,), stride=(1,))\n",
       "    (2): Conv1d(500, 100, kernel_size=(5,), stride=(1,))\n",
       "  )\n",
       "  (fc): Linear(in_features=300, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3987451c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TextCNN                                  --\n",
       "├─Embedding: 1-1                         512,000\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─Conv1d: 2-1                       150,100\n",
       "│    └─Conv1d: 2-2                       200,100\n",
       "│    └─Conv1d: 2-3                       250,100\n",
       "├─Linear: 1-3                            1,505\n",
       "├─Dropout: 1-4                           --\n",
       "=================================================================\n",
       "Total params: 1,113,805\n",
       "Trainable params: 1,113,805\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "4346298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "f2e7e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test functions\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.to(device).train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        acc = accuracy_score(y.cpu().numpy(), predictions.argmax(dim=1).cpu().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "    \n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            predictions = model(X)\n",
    "            loss = criterion(predictions, y)\n",
    "            \n",
    "            acc = accuracy_score(y.cpu().numpy(), predictions.argmax(dim=1).cpu().numpy())\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "    \n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "501f3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 01\n",
      "\tTrain Loss: 1.154 | Train Acc: 66.42%\n",
      "\tTest Loss: 1.073 | Test Acc: 71.22%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.765 | Train Acc: 74.38%\n",
      "\tTest Loss: 1.026 | Test Acc: 71.55%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.595 | Train Acc: 80.71%\n",
      "\tTest Loss: 0.855 | Test Acc: 73.19%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.445 | Train Acc: 84.62%\n",
      "\tTest Loss: 0.958 | Test Acc: 72.86%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.355 | Train Acc: 88.21%\n",
      "\tTest Loss: 0.938 | Test Acc: 73.36%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.305 | Train Acc: 90.08%\n",
      "\tTest Loss: 0.857 | Test Acc: 73.19%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.282 | Train Acc: 90.96%\n",
      "\tTest Loss: 0.904 | Test Acc: 73.03%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.244 | Train Acc: 92.67%\n",
      "\tTest Loss: 1.018 | Test Acc: 73.52%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.194 | Train Acc: 93.54%\n",
      "\tTest Loss: 0.944 | Test Acc: 73.03%\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.184 | Train Acc: 94.08%\n",
      "\tTest Loss: 0.976 | Test Acc: 73.19%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'textcnn_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "2b53ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Token greater than max length, truncating\n",
      "WARNING: Token greater than max length, truncating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [4],\n",
       " [4]]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "y_test = []\n",
    "y_pred = []\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        pred = pred.argmax(dim=1).unsqueeze(dim=1).cpu().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_test.extend(y.unsqueeze(dim=1).cpu().tolist())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "077b070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(592, 592)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test), len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ed9d0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7297297297297297\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.12      0.20        43\n",
      "           1       0.31      0.12      0.17        34\n",
      "           2       0.35      0.14      0.20        43\n",
      "           3       0.33      0.18      0.23        51\n",
      "           4       0.77      0.97      0.86       421\n",
      "\n",
      "    accuracy                           0.73       592\n",
      "   macro avg       0.48      0.30      0.33       592\n",
      "weighted avg       0.67      0.73      0.67       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "bb07aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "53585e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = BPETokenizer.load(\"tokenizer_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "6e31912b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 1024,\n",
       " 'merges': {('e', '</w>'): 'e</w>',\n",
       "  ('t', 'h'): 'th',\n",
       "  ('t', '</w>'): 't</w>',\n",
       "  ('s', '</w>'): 's</w>',\n",
       "  ('d', '</w>'): 'd</w>',\n",
       "  ('.', '</w>'): '.</w>',\n",
       "  ('y', '</w>'): 'y</w>',\n",
       "  ('e', 'r'): 'er',\n",
       "  ('a', 'n'): 'an',\n",
       "  ('i', 'n'): 'in',\n",
       "  ('o', 'r'): 'or',\n",
       "  ('a', 'r'): 'ar',\n",
       "  ('o', '</w>'): 'o</w>',\n",
       "  ('th', 'e</w>'): 'the</w>',\n",
       "  ('an', 'd</w>'): 'and</w>',\n",
       "  ('I', '</w>'): 'I</w>',\n",
       "  ('th', 'e'): 'the',\n",
       "  ('s', 'h'): 'sh',\n",
       "  ('o', 'n'): 'on',\n",
       "  ('a', 'l'): 'al',\n",
       "  ('', '', '</w>'): ',</w>',\n",
       "  ('o', 'u'): 'ou',\n",
       "  ('r', 'e'): 're',\n",
       "  ('f', 'or'): 'for',\n",
       "  ('a', '</w>'): 'a</w>',\n",
       "  ('sh', 'o'): 'sho',\n",
       "  ('o', 'm'): 'om',\n",
       "  ('in', 'g'): 'ing',\n",
       "  ('e', 'n'): 'en',\n",
       "  ('er', '</w>'): 'er</w>',\n",
       "  ('t', 'o</w>'): 'to</w>',\n",
       "  ('ing', '</w>'): 'ing</w>',\n",
       "  ('e', 's</w>'): 'es</w>',\n",
       "  ('i', 's</w>'): 'is</w>',\n",
       "  ('e', 'd</w>'): 'ed</w>',\n",
       "  ('f', '</w>'): 'f</w>',\n",
       "  ('s', 'e</w>'): 'se</w>',\n",
       "  ('T', 'h'): 'Th',\n",
       "  ('h', 'a'): 'ha',\n",
       "  ('o', 'o'): 'oo',\n",
       "  ('v', 'e</w>'): 've</w>',\n",
       "  ('w', 'i'): 'wi',\n",
       "  ('s', 't'): 'st',\n",
       "  ('l', 'i'): 'li',\n",
       "  ('a', 'b'): 'ab',\n",
       "  ('l', '</w>'): 'l</w>',\n",
       "  ('l', 'e</w>'): 'le</w>',\n",
       "  ('l', 'y</w>'): 'ly</w>',\n",
       "  ('c', 'om'): 'com',\n",
       "  ('a', 't</w>'): 'at</w>',\n",
       "  ('s', 'i'): 'si',\n",
       "  ('m', '</w>'): 'm</w>',\n",
       "  ('e', '.</w>'): 'e.</w>',\n",
       "  ('w', 'e'): 'we',\n",
       "  ('o', 'f</w>'): 'of</w>',\n",
       "  ('for', '</w>'): 'for</w>',\n",
       "  ('g', 'h'): 'gh',\n",
       "  ('in', '</w>'): 'in</w>',\n",
       "  ('f', 'e'): 'fe',\n",
       "  ('ar', 'e</w>'): 'are</w>',\n",
       "  ('a', 's</w>'): 'as</w>',\n",
       "  ('t', 'i'): 'ti',\n",
       "  ('b', 'u'): 'bu',\n",
       "  ('i', 't</w>'): 'it</w>',\n",
       "  ('k', '</w>'): 'k</w>',\n",
       "  ('e', 's'): 'es',\n",
       "  ('on', '</w>'): 'on</w>',\n",
       "  ('for', 't'): 'fort',\n",
       "  ('v', 'er'): 'ver',\n",
       "  ('i', 't'): 'it',\n",
       "  ('m', 'y</w>'): 'my</w>',\n",
       "  ('c', 'h'): 'ch',\n",
       "  ('th', '</w>'): 'th</w>',\n",
       "  ('s', 'u'): 'su',\n",
       "  ('sho', 'es</w>'): 'shoes</w>',\n",
       "  ('the', 'y</w>'): 'they</w>',\n",
       "  ('i', 'r'): 'ir',\n",
       "  ('fort', 'ab'): 'fortab',\n",
       "  ('a', 't'): 'at',\n",
       "  ('o', 'w'): 'ow',\n",
       "  ('e', 'a'): 'ea',\n",
       "  ('com', 'fortab'): 'comfortab',\n",
       "  ('the', 'se</w>'): 'these</w>',\n",
       "  ('s', 't</w>'): 'st</w>',\n",
       "  ('a', 'c'): 'ac',\n",
       "  ('bu', 't</w>'): 'but</w>',\n",
       "  ('an', '</w>'): 'an</w>',\n",
       "  ('Th', 'e'): 'The',\n",
       "  ('ha', 've</w>'): 'have</w>',\n",
       "  ('!', '</w>'): '!</w>',\n",
       "  ('si', 'z'): 'siz',\n",
       "  ('i', 'c'): 'ic',\n",
       "  ('p', 'p'): 'pp',\n",
       "  ('o', 't</w>'): 'ot</w>',\n",
       "  ('we', 'ar'): 'wear',\n",
       "  ('o', 'l'): 'ol',\n",
       "  ('p', 'a'): 'pa',\n",
       "  ('wi', 'th</w>'): 'with</w>',\n",
       "  ('the', 'm</w>'): 'them</w>',\n",
       "  ('l', 'oo'): 'loo',\n",
       "  ('b', 'e'): 'be',\n",
       "  ('en', '</w>'): 'en</w>',\n",
       "  ('l', 'l</w>'): 'll</w>',\n",
       "  ('sho', 'e</w>'): 'shoe</w>',\n",
       "  ('fe', 'e'): 'fee',\n",
       "  ('t', 'o'): 'to',\n",
       "  ('n', '</w>'): 'n</w>',\n",
       "  ('u', 'r'): 'ur',\n",
       "  ('th', 'at</w>'): 'that</w>',\n",
       "  ('s', 'o</w>'): 'so</w>',\n",
       "  ('Th', 'e</w>'): 'The</w>',\n",
       "  ('e', '', '</w>'): 'e,</w>',\n",
       "  ('p', '</w>'): 'p</w>',\n",
       "  ('w', 'h'): 'wh',\n",
       "  ('k', 'e</w>'): 'ke</w>',\n",
       "  ('s', '.</w>'): 's.</w>',\n",
       "  ('gh', 't</w>'): 'ght</w>',\n",
       "  ('d', 'i'): 'di',\n",
       "  ('w', 'as</w>'): 'was</w>',\n",
       "  ('ver', 'y</w>'): 'very</w>',\n",
       "  ('g', 're'): 'gre',\n",
       "  ('pa', 'ir'): 'pair',\n",
       "  ('r', 'i'): 'ri',\n",
       "  ('comfortab', 'le</w>'): 'comfortable</w>',\n",
       "  ('t', '.</w>'): 't.</w>',\n",
       "  ('l', 'o'): 'lo',\n",
       "  ('siz', 'e</w>'): 'size</w>',\n",
       "  ('al', 'ly</w>'): 'ally</w>',\n",
       "  ('y', 'ou'): 'you',\n",
       "  ('al', '</w>'): 'al</w>',\n",
       "  ('d', 'e</w>'): 'de</w>',\n",
       "  ('n', 'ot</w>'): 'not</w>',\n",
       "  ('or', '</w>'): 'or</w>',\n",
       "  ('ch', '</w>'): 'ch</w>',\n",
       "  ('t', 't'): 'tt',\n",
       "  ('s', 'e'): 'se',\n",
       "  ('n', 'e'): 'ne',\n",
       "  ('ou', 'l'): 'oul',\n",
       "  ('er', 'e</w>'): 'ere</w>',\n",
       "  ('ou', 't</w>'): 'out</w>',\n",
       "  ('w', 'or'): 'wor',\n",
       "  ('r', 'o'): 'ro',\n",
       "  ('th', 'is</w>'): 'this</w>',\n",
       "  ('m', 'e</w>'): 'me</w>',\n",
       "  ('c', 'a'): 'ca',\n",
       "  ('wear', '</w>'): 'wear</w>',\n",
       "  ('d', 'a'): 'da',\n",
       "  ('m', 'a'): 'ma',\n",
       "  ('b', 'e</w>'): 'be</w>',\n",
       "  ('f', 'it</w>'): 'fit</w>',\n",
       "  ('li', 'ke</w>'): 'like</w>',\n",
       "  ('th', 'er</w>'): 'ther</w>',\n",
       "  ('ow', '</w>'): 'ow</w>',\n",
       "  ('e', 'x'): 'ex',\n",
       "  ('r', 'u'): 'ru',\n",
       "  ('sho', 'es'): 'shoes',\n",
       "  ('s', 'o'): 'so',\n",
       "  ('p', 'er'): 'per',\n",
       "  ('d', 'e'): 'de',\n",
       "  (\"'\", 't</w>'): \"'t</w>\",\n",
       "  ('The', 'y</w>'): 'They</w>',\n",
       "  ('oul', 'd</w>'): 'ould</w>',\n",
       "  ('I', \"'\"): \"I'\",\n",
       "  ('l', 'e'): 'le',\n",
       "  ('k', 'ing</w>'): 'king</w>',\n",
       "  ('st', 'y'): 'sty',\n",
       "  ('d', '.</w>'): 'd.</w>',\n",
       "  ('al', 'l</w>'): 'all</w>',\n",
       "  ('pair', '</w>'): 'pair</w>',\n",
       "  ('y', '.</w>'): 'y.</w>',\n",
       "  ('s', 'a'): 'sa',\n",
       "  ('q', 'u'): 'qu',\n",
       "  ('l', 'ac'): 'lac',\n",
       "  ('l', 'a'): 'la',\n",
       "  ('The', 'se</w>'): 'These</w>',\n",
       "  ('m', 'en'): 'men',\n",
       "  ('gre', 'at</w>'): 'great</w>',\n",
       "  ('m', 'o'): 'mo',\n",
       "  ('b', 'o'): 'bo',\n",
       "  ('l', 'e.</w>'): 'le.</w>',\n",
       "  (\"'\", 's</w>'): \"'s</w>\",\n",
       "  ('e', 'l'): 'el',\n",
       "  ('s', 'n'): 'sn',\n",
       "  ('u', 'n'): 'un',\n",
       "  ('fe', 'c'): 'fec',\n",
       "  ('g', 'e'): 'ge',\n",
       "  ('you', '</w>'): 'you</w>',\n",
       "  ('oo', 'd</w>'): 'ood</w>',\n",
       "  ('d', 'er'): 'der',\n",
       "  ('k', 's</w>'): 'ks</w>',\n",
       "  ('u', 'st</w>'): 'ust</w>',\n",
       "  ('or', 'e</w>'): 'ore</w>',\n",
       "  ('w', 'al'): 'wal',\n",
       "  ('to', 'o</w>'): 'too</w>',\n",
       "  ('w', 'a'): 'wa',\n",
       "  ('th', 'an</w>'): 'than</w>',\n",
       "  ('om', '</w>'): 'om</w>',\n",
       "  ('loo', 'k</w>'): 'look</w>',\n",
       "  ('on', 'g'): 'ong',\n",
       "  ('w', 'ere</w>'): 'were</w>',\n",
       "  ('f', 'f'): 'ff',\n",
       "  ('the', 'm'): 'them',\n",
       "  ('t', 'r'): 'tr',\n",
       "  ('c', 'on'): 'con',\n",
       "  ('su', 'pp'): 'supp',\n",
       "  ('h', 'i'): 'hi',\n",
       "  ('w', '</w>'): 'w</w>',\n",
       "  ('gh', '</w>'): 'gh</w>',\n",
       "  ('ea', 'k'): 'eak',\n",
       "  ('u', 'se</w>'): 'use</w>',\n",
       "  ('p', 'e'): 'pe',\n",
       "  ('f', 'oo'): 'foo',\n",
       "  ('f', 'r'): 'fr',\n",
       "  ('e', 'ar'): 'ear',\n",
       "  ('e', 'ly</w>'): 'ely</w>',\n",
       "  ('al', 'l'): 'all',\n",
       "  ('v', 'er</w>'): 'ver</w>',\n",
       "  ('en', 't</w>'): 'ent</w>',\n",
       "  ('c', 'u'): 'cu',\n",
       "  ('supp', 'or'): 'suppor',\n",
       "  ('re', 'a'): 'rea',\n",
       "  ('o', 'ther</w>'): 'other</w>',\n",
       "  ('s', '', '</w>'): 's,</w>',\n",
       "  ('ha', 'd</w>'): 'had</w>',\n",
       "  ('f', 'in'): 'fin',\n",
       "  ('c', 'ol'): 'col',\n",
       "  ('s', 'li'): 'sli',\n",
       "  ('per', 'fec'): 'perfec',\n",
       "  ('j', 'ust</w>'): 'just</w>',\n",
       "  ('s', 'ed</w>'): 'sed</w>',\n",
       "  ('m', 'ore</w>'): 'more</w>',\n",
       "  ('shoes', '.</w>'): 'shoes.</w>',\n",
       "  ('fee', 't</w>'): 'feet</w>',\n",
       "  ('w', 'ould</w>'): 'would</w>',\n",
       "  ('v', 'i'): 'vi',\n",
       "  ('on', 'e</w>'): 'one</w>',\n",
       "  ('ou', 'ght</w>'): 'ought</w>',\n",
       "  ('lo', 've</w>'): 'love</w>',\n",
       "  ('sn', 'eak'): 'sneak',\n",
       "  ('ic', 'e</w>'): 'ice</w>',\n",
       "  ('r', 'e</w>'): 're</w>',\n",
       "  ('ou', 'n'): 'oun',\n",
       "  ('.', '.'): '..',\n",
       "  ('er', 's</w>'): 'ers</w>',\n",
       "  ('er', 'y</w>'): 'ery</w>',\n",
       "  ('wi', 'd'): 'wid',\n",
       "  ('g', 'ood</w>'): 'good</w>',\n",
       "  ('l', 'ea'): 'lea',\n",
       "  ('s', 'om'): 'som',\n",
       "  ('t', '', '</w>'): 't,</w>',\n",
       "  ('p', 'ro'): 'pro',\n",
       "  ('k', 'e'): 'ke',\n",
       "  ('t', 'er</w>'): 'ter</w>',\n",
       "  ('s', 'ol'): 'sol',\n",
       "  ('wi', 'de</w>'): 'wide</w>',\n",
       "  ('sho', 'e.</w>'): 'shoe.</w>',\n",
       "  ('l', 'ar'): 'lar',\n",
       "  ('u', 'p</w>'): 'up</w>',\n",
       "  ('V', 'ery</w>'): 'Very</w>',\n",
       "  ('ge', 't</w>'): 'get</w>',\n",
       "  ('c', 'e</w>'): 'ce</w>',\n",
       "  ('5', '</w>'): '5</w>',\n",
       "  ('G', 're'): 'Gre',\n",
       "  ('fr', 'om</w>'): 'from</w>',\n",
       "  ('t', 'ur'): 'tur',\n",
       "  ('y', '', '</w>'): 'y,</w>',\n",
       "  ('Gre', 'at</w>'): 'Great</w>',\n",
       "  ('ing', '.</w>'): 'ing.</w>',\n",
       "  ('f', 'it'): 'fit',\n",
       "  ('da', 'y</w>'): 'day</w>',\n",
       "  ('l', 'e', '</w>'): 'le,</w>',\n",
       "  ('or', 'der'): 'order',\n",
       "  ('ar', 'r'): 'arr',\n",
       "  ('s', 'm'): 'sm',\n",
       "  ('wi', 'll</w>'): 'will</w>',\n",
       "  ('a', 'g'): 'ag',\n",
       "  ('wh', 'i'): 'whi',\n",
       "  ('comfortab', 'le.</w>'): 'comfortable.</w>',\n",
       "  ('al', 'it'): 'alit',\n",
       "  (\"I'\", 'm</w>'): \"I'm</w>\",\n",
       "  ('h', 'o'): 'ho',\n",
       "  ('2', '</w>'): '2</w>',\n",
       "  ('t', 's</w>'): 'ts</w>',\n",
       "  ('them', '.</w>'): 'them.</w>',\n",
       "  ('i', 'g'): 'ig',\n",
       "  ('y', 's</w>'): 'ys</w>',\n",
       "  ('mo', 'st</w>'): 'most</w>',\n",
       "  ('M', 'y</w>'): 'My</w>',\n",
       "  ('er', '.</w>'): 'er.</w>',\n",
       "  ('u', 's'): 'us',\n",
       "  ('re', 'ally</w>'): 'really</w>',\n",
       "  ('c', 'an</w>'): 'can</w>',\n",
       "  ('ca', 'use</w>'): 'cause</w>',\n",
       "  ('n', 'arr'): 'narr',\n",
       "  ('y', 'ear'): 'year',\n",
       "  ('qu', 'alit'): 'qualit',\n",
       "  ('ou', 'gh</w>'): 'ough</w>',\n",
       "  ('Th', 'is</w>'): 'This</w>',\n",
       "  ('d', 's</w>'): 'ds</w>',\n",
       "  ('r', '</w>'): 'r</w>',\n",
       "  ('p', 're'): 'pre',\n",
       "  ('gre', 'at'): 'great',\n",
       "  ('t', 'ed</w>'): 'ted</w>',\n",
       "  ('fee', 'l</w>'): 'feel</w>',\n",
       "  ('m', 'i'): 'mi',\n",
       "  ('st', 'i'): 'sti',\n",
       "  ('a', 'm</w>'): 'am</w>',\n",
       "  ('com', 'p'): 'comp',\n",
       "  ('u', 't</w>'): 'ut</w>',\n",
       "  ('c', 'ha'): 'cha',\n",
       "  ('ha', 's</w>'): 'has</w>',\n",
       "  ('p', 'ur'): 'pur',\n",
       "  ('h', 'e'): 'he',\n",
       "  ('loo', 'king</w>'): 'looking</w>',\n",
       "  ('sh', '</w>'): 'sh</w>',\n",
       "  ('l', 'ong'): 'long',\n",
       "  ('f', 'ter</w>'): 'fter</w>',\n",
       "  ('t', 'a'): 'ta',\n",
       "  ('wh', 'en</w>'): 'when</w>',\n",
       "  ('1', '0'): '10',\n",
       "  ('i', 'f</w>'): 'if</w>',\n",
       "  ('d', 'o'): 'do',\n",
       "  ('f', 'ir'): 'fir',\n",
       "  ('li', 'tt'): 'litt',\n",
       "  ('ab', 'out</w>'): 'about</w>',\n",
       "  ('m', 'u'): 'mu',\n",
       "  ('be', 'cause</w>'): 'because</w>',\n",
       "  ('u', 'e</w>'): 'ue</w>',\n",
       "  ('p', 'er</w>'): 'per</w>',\n",
       "  ('ow', 'n</w>'): 'own</w>',\n",
       "  ('a', 'pp'): 'app',\n",
       "  ('d', 'on'): 'don',\n",
       "  ('it', 'e</w>'): 'ite</w>',\n",
       "  ('p', 'r'): 'pr',\n",
       "  ('oun', 'd</w>'): 'ound</w>',\n",
       "  ('litt', 'le</w>'): 'little</w>',\n",
       "  ('so', 'le</w>'): 'sole</w>',\n",
       "  ('b', 'ought</w>'): 'bought</w>',\n",
       "  ('l', 'l'): 'll',\n",
       "  ('pur', 'cha'): 'purcha',\n",
       "  ('ti', 'on'): 'tion',\n",
       "  ('d', '', '</w>'): 'd,</w>',\n",
       "  ('t', 'e'): 'te',\n",
       "  ('es', '.</w>'): 'es.</w>',\n",
       "  (\"I'\", 've</w>'): \"I've</w>\",\n",
       "  ('order', 'ed</w>'): 'ordered</w>',\n",
       "  ('t', 'ly</w>'): 'tly</w>',\n",
       "  ('n', 'o</w>'): 'no</w>',\n",
       "  ('su', 'al</w>'): 'sual</w>',\n",
       "  ('b', 'ig'): 'big',\n",
       "  ('b', 'r'): 'br',\n",
       "  ('wear', 'ing</w>'): 'wearing</w>',\n",
       "  ('com', 'men'): 'commen',\n",
       "  ('be', 'en</w>'): 'been</w>',\n",
       "  ('f', 'u'): 'fu',\n",
       "  ('pe', 'c'): 'pec',\n",
       "  ('m', 'e'): 'me',\n",
       "  ('d', 'er</w>'): 'der</w>',\n",
       "  ('f', 'a'): 'fa',\n",
       "  (')', '</w>'): ')</w>',\n",
       "  ('d', 'ed</w>'): 'ded</w>',\n",
       "  ('re', 'tur'): 'retur',\n",
       "  ('sty', 'le</w>'): 'style</w>',\n",
       "  ('an', 'y</w>'): 'any</w>',\n",
       "  ('.', '5</w>'): '.5</w>',\n",
       "  ('L', 'o'): 'Lo',\n",
       "  ('foo', 't</w>'): 'foot</w>',\n",
       "  ('ab', 'le</w>'): 'able</w>',\n",
       "  ('whi', 'ch</w>'): 'which</w>',\n",
       "  ('-', '</w>'): '-</w>',\n",
       "  ('bo', 'x'): 'box',\n",
       "  ('ti', 'c'): 'tic',\n",
       "  ('e', 'v'): 'ev',\n",
       "  (\"'\", 're</w>'): \"'re</w>\",\n",
       "  ('d', 'n'): 'dn',\n",
       "  ('we', 'll</w>'): 'well</w>',\n",
       "  ('at', 'er'): 'ater',\n",
       "  ('som', 'e</w>'): 'some</w>',\n",
       "  ('ti', 'on</w>'): 'tion</w>',\n",
       "  ('th', 'ing</w>'): 'thing</w>',\n",
       "  ('d', 'o</w>'): 'do</w>',\n",
       "  ('h', 'e</w>'): 'he</w>',\n",
       "  ('p', 'o'): 'po',\n",
       "  ('m', 'on'): 'mon',\n",
       "  ('e', 'ver'): 'ever',\n",
       "  ('b', 'ac'): 'bac',\n",
       "  ('l', 't</w>'): 'lt</w>',\n",
       "  ('suppor', 't</w>'): 'support</w>',\n",
       "  ('ar', 'ch</w>'): 'arch</w>',\n",
       "  ('sh', 'i'): 'shi',\n",
       "  ('fir', 'st</w>'): 'first</w>',\n",
       "  ('r', 'a'): 'ra',\n",
       "  ('re', 'c'): 'rec',\n",
       "  ('g', 'o'): 'go',\n",
       "  ('k', '.</w>'): 'k.</w>',\n",
       "  ('d', 'ing</w>'): 'ding</w>',\n",
       "  ('re', 'commen'): 'recommen',\n",
       "  ('k', 'ed</w>'): 'ked</w>',\n",
       "  ('es', 's</w>'): 'ess</w>',\n",
       "  ('n', 'ow</w>'): 'now</w>',\n",
       "  ('a', 'fter</w>'): 'after</w>',\n",
       "  ('v', 'ed</w>'): 'ved</w>',\n",
       "  ('o', 'p'): 'op',\n",
       "  ('di', 'ff'): 'diff',\n",
       "  ('s', 'y</w>'): 'sy</w>',\n",
       "  ('e', 'p'): 'ep',\n",
       "  ('t', 'ru'): 'tru',\n",
       "  ('ca', 'sual</w>'): 'casual</w>',\n",
       "  ('lar', 'g'): 'larg',\n",
       "  ('col', 'or'): 'color',\n",
       "  ('b', 'it</w>'): 'bit</w>',\n",
       "  ('narr', 'ow</w>'): 'narrow</w>',\n",
       "  ('qualit', 'y</w>'): 'quality</w>',\n",
       "  ('s', 'e.</w>'): 'se.</w>',\n",
       "  ('on', 'ly</w>'): 'only</w>',\n",
       "  ('at', 'e</w>'): 'ate</w>',\n",
       "  ('to', 'e</w>'): 'toe</w>',\n",
       "  ('you', 'r</w>'): 'your</w>',\n",
       "  ('ri', 'ght</w>'): 'right</w>',\n",
       "  ('ti', 'me</w>'): 'time</w>',\n",
       "  ('Lo', 've</w>'): 'Love</w>',\n",
       "  ('sli', 'p</w>'): 'slip</w>',\n",
       "  ('g', 'ot</w>'): 'got</w>',\n",
       "  ('b', 'rea'): 'brea',\n",
       "  ('don', \"'t</w>\"): \"don't</w>\",\n",
       "  ('h', 'al'): 'hal',\n",
       "  ('f', 'f</w>'): 'ff</w>',\n",
       "  ('g', 'o</w>'): 'go</w>',\n",
       "  ('H', 'e</w>'): 'He</w>',\n",
       "  ('sty', 'li'): 'styli',\n",
       "  ('I', 't</w>'): 'It</w>',\n",
       "  ('p', 'ar'): 'par',\n",
       "  ('n', 'o'): 'no',\n",
       "  ('e', 'l</w>'): 'el</w>',\n",
       "  ('w', 'ha'): 'wha',\n",
       "  ('ever', 'y'): 'every',\n",
       "  ('be', 'tt'): 'bett',\n",
       "  ('or', 'm'): 'orm',\n",
       "  ('a', 'd'): 'ad',\n",
       "  ('bu', 'y</w>'): 'buy</w>',\n",
       "  ('mu', 'ch</w>'): 'much</w>',\n",
       "  ('1', '1'): '11',\n",
       "  ('sti', 'll</w>'): 'still</w>',\n",
       "  ('o', 'ff'): 'off',\n",
       "  ('so', 'c'): 'soc',\n",
       "  ('s', 'k'): 'sk',\n",
       "  ('y', \"'re</w>\"): \"y're</w>\",\n",
       "  ('sm', 'all'): 'small',\n",
       "  ('wor', 'k</w>'): 'work</w>',\n",
       "  ('wor', 'n</w>'): 'worn</w>',\n",
       "  ('u', 'c'): 'uc',\n",
       "  ('wa', 'y</w>'): 'way</w>',\n",
       "  ('us', 'b'): 'usb',\n",
       "  ('es', 't</w>'): 'est</w>',\n",
       "  ('wal', 'king</w>'): 'walking</w>',\n",
       "  ('I', '’'): 'I’',\n",
       "  ('!', '!</w>'): '!!</w>',\n",
       "  ('long', '</w>'): 'long</w>',\n",
       "  ('hal', 'f</w>'): 'half</w>',\n",
       "  ('ar', '</w>'): 'ar</w>',\n",
       "  ('perfec', 't</w>'): 'perfect</w>',\n",
       "  ('’', 't</w>'): '’t</w>',\n",
       "  ('siz', 'e.</w>'): 'size.</w>',\n",
       "  ('n', 'ice</w>'): 'nice</w>',\n",
       "  ('n', 'orm'): 'norm',\n",
       "  ('ar', 'd</w>'): 'ard</w>',\n",
       "  ('ev', 'en</w>'): 'even</w>',\n",
       "  ('an', 's</w>'): 'ans</w>',\n",
       "  ('m', 'e.</w>'): 'me.</w>',\n",
       "  ('fee', 'l'): 'feel',\n",
       "  ('bac', 'k</w>'): 'back</w>',\n",
       "  ('ic', '</w>'): 'ic</w>',\n",
       "  ('tru', 'e</w>'): 'true</w>',\n",
       "  ('diff', 'er'): 'differ',\n",
       "  ('g', 'i'): 'gi',\n",
       "  ('e', 'i'): 'ei',\n",
       "  ('ater', 'i'): 'ateri',\n",
       "  ('w', 'an'): 'wan',\n",
       "  ('comfortab', 'le', '</w>'): 'comfortable,</w>',\n",
       "  ('h', 'usb'): 'husb',\n",
       "  ('col', 'or</w>'): 'color</w>',\n",
       "  ('se', 'e'): 'see',\n",
       "  ('wha', 't</w>'): 'what</w>',\n",
       "  ('b', 'lac'): 'blac',\n",
       "  ('\"', '</w>'): '\"</w>',\n",
       "  ('p', 'ut</w>'): 'put</w>',\n",
       "  ('dn', \"'t</w>\"): \"dn't</w>\",\n",
       "  ('si', 'g'): 'sig',\n",
       "  ('al', 'so</w>'): 'also</w>',\n",
       "  ('’', 's</w>'): '’s</w>',\n",
       "  ('lea', 'ther</w>'): 'leather</w>',\n",
       "  ('y', 'ing</w>'): 'ying</w>',\n",
       "  ('great', '.</w>'): 'great.</w>',\n",
       "  ('ar', 'ound</w>'): 'around</w>',\n",
       "  ('k', 'es</w>'): 'kes</w>',\n",
       "  ('C', 'om'): 'Com',\n",
       "  ('an', 'other</w>'): 'another</w>',\n",
       "  ('t', 'ri'): 'tri',\n",
       "  ('for', 't</w>'): 'fort</w>',\n",
       "  ('ol', 'd</w>'): 'old</w>',\n",
       "  ('m', 'ateri'): 'materi',\n",
       "  ('app', 'o'): 'appo',\n",
       "  ('ha', 'pp'): 'happ',\n",
       "  ('i', 'm'): 'im',\n",
       "  ('c', 'an'): 'can',\n",
       "  ('in', '.</w>'): 'in.</w>',\n",
       "  ('wi', 'th'): 'with',\n",
       "  ('el', 'l'): 'ell',\n",
       "  ('t', 'u'): 'tu',\n",
       "  ('h', 'ow</w>'): 'how</w>',\n",
       "  ('an', 'd'): 'and',\n",
       "  ('th', 'er'): 'ther',\n",
       "  ('Z', 'appo'): 'Zappo',\n",
       "  ('v', 'es</w>'): 'ves</w>',\n",
       "  ('su', 'per</w>'): 'super</w>',\n",
       "  ('fe', 'lt</w>'): 'felt</w>',\n",
       "  ('l', 'u'): 'lu',\n",
       "  ('t', 'er'): 'ter',\n",
       "  ('we', 'll'): 'well',\n",
       "  ('si', 'de</w>'): 'side</w>',\n",
       "  ('ne', 'w</w>'): 'new</w>',\n",
       "  ('f', 't</w>'): 'ft</w>',\n",
       "  ('3', '</w>'): '3</w>',\n",
       "  ('n', 'ed</w>'): 'ned</w>',\n",
       "  ('ex', 'c'): 'exc',\n",
       "  ('shoes', '', '</w>'): 'shoes,</w>',\n",
       "  ('on', '.</w>'): 'on.</w>',\n",
       "  ('t', 'oo'): 'too',\n",
       "  ('s', 'si'): 'ssi',\n",
       "  ('s', 's</w>'): 'ss</w>',\n",
       "  ('t', 'e</w>'): 'te</w>',\n",
       "  ('e', 'ver</w>'): 'ever</w>',\n",
       "  ('we', 'e'): 'wee',\n",
       "  ('sol', 'es</w>'): 'soles</w>',\n",
       "  ('sneak', 'ers</w>'): 'sneakers</w>',\n",
       "  ('h', 'is</w>'): 'his</w>',\n",
       "  ('an', 'g'): 'ang',\n",
       "  ('s', 'in'): 'sin',\n",
       "  ('fee', 't.</w>'): 'feet.</w>',\n",
       "  ('s', 'er'): 'ser',\n",
       "  ('sho', 'e', '</w>'): 'shoe,</w>',\n",
       "  ('ea', 'sy</w>'): 'easy</w>',\n",
       "  ('i', 's'): 'is',\n",
       "  ('pair', 's</w>'): 'pairs</w>',\n",
       "  ('gh', 't'): 'ght',\n",
       "  ('bett', 'er</w>'): 'better</w>',\n",
       "  ('recommen', 'd</w>'): 'recommend</w>',\n",
       "  ('an', 'y'): 'any',\n",
       "  ('ma', 'de</w>'): 'made</w>',\n",
       "  ('differ', 'ent</w>'): 'different</w>',\n",
       "  ('com', 'fort</w>'): 'comfort</w>',\n",
       "  ('sa', 'me</w>'): 'same</w>',\n",
       "  ('g', 'u'): 'gu',\n",
       "  ('i', '</w>'): 'i</w>',\n",
       "  ('ag', 'a'): 'aga',\n",
       "  ('husb', 'and</w>'): 'husband</w>',\n",
       "  ('box', '</w>'): 'box</w>',\n",
       "  ('pro', 'b'): 'prob',\n",
       "  ('mon', 'th'): 'month',\n",
       "  ('w', 'om'): 'wom',\n",
       "  ('ex', 'pec'): 'expec',\n",
       "  ('ir', '</w>'): 'ir</w>',\n",
       "  ('t', 're'): 'tre',\n",
       "  ('wal', 'k</w>'): 'walk</w>',\n",
       "  ('vi', 'e'): 'vie',\n",
       "  ('E', '</w>'): 'E</w>',\n",
       "  ('i', 'l'): 'il',\n",
       "  ('th', 'o'): 'tho',\n",
       "  ('de', 'sig'): 'desig',\n",
       "  ('wa', 'ys</w>'): 'ways</w>',\n",
       "  ('wid', 'th</w>'): 'width</w>',\n",
       "  ('ru', 'n</w>'): 'run</w>',\n",
       "  ('h', 'ou'): 'hou',\n",
       "  ('lac', 'es</w>'): 'laces</w>',\n",
       "  ('ne', 'e'): 'nee',\n",
       "  ('u', 'p'): 'up',\n",
       "  ('b', 'y</w>'): 'by</w>',\n",
       "  ('d', 'y</w>'): 'dy</w>',\n",
       "  ('or', 'der</w>'): 'order</w>',\n",
       "  ('it', \"'s</w>\"): \"it's</w>\",\n",
       "  ('la', 'st</w>'): 'last</w>',\n",
       "  ('t', 'y'): 'ty',\n",
       "  ('S', 'o</w>'): 'So</w>',\n",
       "  ('S', '</w>'): 'S</w>',\n",
       "  ('blac', 'k</w>'): 'black</w>',\n",
       "  ('ne', 'ed</w>'): 'need</w>',\n",
       "  ('n', '.</w>'): 'n.</w>',\n",
       "  ('b', 'a'): 'ba',\n",
       "  ('ou', 't'): 'out',\n",
       "  ('c', 'ould</w>'): 'could</w>',\n",
       "  ('som', 'e'): 'some',\n",
       "  ('p', 'ic'): 'pic',\n",
       "  ('hi', 'gh</w>'): 'high</w>',\n",
       "  ('re', 'vie'): 'revie',\n",
       "  ('we', 'i'): 'wei',\n",
       "  ('s', 'p'): 'sp',\n",
       "  ('en', 'd</w>'): 'end</w>',\n",
       "  ('d', 'own</w>'): 'down</w>',\n",
       "  ('li', 'ght</w>'): 'light</w>',\n",
       "  ('fin', 'd</w>'): 'find</w>',\n",
       "  ('e', 'p</w>'): 'ep</w>',\n",
       "  ('th', 'ere</w>'): 'there</w>',\n",
       "  ('loo', 'ks</w>'): 'looks</w>',\n",
       "  ('R', 'e'): 'Re',\n",
       "  ('o', 'ver</w>'): 'over</w>',\n",
       "  ('he', 'el</w>'): 'heel</w>',\n",
       "  ('S', 'ke'): 'Ske',\n",
       "  ('d', 're'): 'dre',\n",
       "  ('v', 'ing</w>'): 'ving</w>',\n",
       "  ('I', 'f</w>'): 'If</w>',\n",
       "  ('W', 'i'): 'Wi',\n",
       "  ('t', 'l'): 'tl',\n",
       "  ('st', 'ar'): 'star',\n",
       "  (')', '.</w>'): ').</w>',\n",
       "  ('ma', 'ke</w>'): 'make</w>',\n",
       "  ('t', 'ong'): 'tong',\n",
       "  ('cu', 'shi'): 'cushi',\n",
       "  ('g', 'oo'): 'goo',\n",
       "  ('wor', 'e</w>'): 'wore</w>',\n",
       "  ('en', 'ough</w>'): 'enough</w>',\n",
       "  ('s', 'on</w>'): 'son</w>',\n",
       "  ('es', '', '</w>'): 'es,</w>',\n",
       "  ('1', '/'): '1/',\n",
       "  ('o', 'ff</w>'): 'off</w>',\n",
       "  ('purcha', 'sed</w>'): 'purchased</w>',\n",
       "  ('an', 'k'): 'ank',\n",
       "  ('w', 'o</w>'): 'wo</w>',\n",
       "  ('th', 'in'): 'thin',\n",
       "  ('p', 'lac'): 'plac',\n",
       "  ('with', 'out</w>'): 'without</w>',\n",
       "  ('retur', 'n</w>'): 'return</w>',\n",
       "  ('ou', 'gh'): 'ough',\n",
       "  ('S', 'u'): 'Su',\n",
       "  ('ru', 'n'): 'run',\n",
       "  ('ing', '', '</w>'): 'ing,</w>',\n",
       "  ('di', 'd</w>'): 'did</w>',\n",
       "  ('al', 'ways</w>'): 'always</w>',\n",
       "  ('A', 'l'): 'Al',\n",
       "  ('S', 'ho'): 'Sho',\n",
       "  ('su', 'r'): 'sur',\n",
       "  ('fin', 'it'): 'finit',\n",
       "  ('finit', 'ely</w>'): 'finitely</w>',\n",
       "  ('..', '.</w>'): '...</w>',\n",
       "  ('wal', 'k'): 'walk',\n",
       "  ('fe', 'w</w>'): 'few</w>',\n",
       "  ('year', 's</w>'): 'years</w>',\n",
       "  ('suppor', 't.</w>'): 'support.</w>',\n",
       "  ('sho', 'e'): 'shoe',\n",
       "  ('f', 'l'): 'fl',\n",
       "  ('n', 'ing</w>'): 'ning</w>',\n",
       "  ('A', 'n'): 'An',\n",
       "  ('ma', 'z'): 'maz',\n",
       "  ('e', 'd.</w>'): 'ed.</w>',\n",
       "  ('G', 'ood</w>'): 'Good</w>',\n",
       "  ('N', 'ot</w>'): 'Not</w>',\n",
       "  ('wh', 'ite</w>'): 'white</w>',\n",
       "  ('H', 'a'): 'Ha',\n",
       "  ('big', '</w>'): 'big</w>',\n",
       "  ('th', 'r'): 'thr',\n",
       "  ('fit', '.</w>'): 'fit.</w>',\n",
       "  ('tr', 'y</w>'): 'try</w>',\n",
       "  ('0', '</w>'): '0</w>',\n",
       "  ('e', 'very</w>'): 'every</w>',\n",
       "  ('ou', 's</w>'): 'ous</w>',\n",
       "  ('sneak', 'er'): 'sneaker',\n",
       "  ('d', 'ur'): 'dur',\n",
       "  ('A', '</w>'): 'A</w>',\n",
       "  ('l', 'ess</w>'): 'less</w>',\n",
       "  ('B', 'ut</w>'): 'But</w>',\n",
       "  ('la', 'st'): 'last',\n",
       "  ('tt', 'ing</w>'): 'tting</w>',\n",
       "  ('lo', 'ves</w>'): 'loves</w>',\n",
       "  ('tt', 'y</w>'): 'tty</w>',\n",
       "  ('styli', 'sh</w>'): 'stylish</w>',\n",
       "  ('I’', 'm</w>'): 'I’m</w>',\n",
       "  ('go', 'ing</w>'): 'going</w>',\n",
       "  ('happ', 'y</w>'): 'happy</w>',\n",
       "  ('in', 'to</w>'): 'into</w>',\n",
       "  ('10', '</w>'): '10</w>',\n",
       "  ('Com', 'fortab'): 'Comfortab',\n",
       "  ('the', 'ir</w>'): 'their</w>',\n",
       "  ('sneak', 'er</w>'): 'sneaker</w>',\n",
       "  ('t', 'wo</w>'): 'two</w>',\n",
       "  ('ch', 'ers</w>'): 'chers</w>',\n",
       "  ('o', 'c'): 'oc',\n",
       "  ('.', '5'): '.5',\n",
       "  ('com', 'f'): 'comf',\n",
       "  ('m', 'in'): 'min',\n",
       "  ('ne', 'ver</w>'): 'never</w>',\n",
       "  ('n', 'i'): 'ni',\n",
       "  ('i', 'd</w>'): 'id</w>',\n",
       "  ('an', 'ce</w>'): 'ance</w>',\n",
       "  ('narr', 'ow'): 'narrow',\n",
       "  ('H', 'o'): 'Ho',\n",
       "  ('y', 'e'): 'ye',\n",
       "  ('wear', '.</w>'): 'wear.</w>',\n",
       "  ('is', 'su'): 'issu',\n",
       "  ('feel', 's</w>'): 'feels</w>',\n",
       "  ('t', 'ing</w>'): 'ting</w>',\n",
       "  ('pp', 'o'): 'ppo',\n",
       "  ('ti', 've</w>'): 'tive</w>',\n",
       "  ('k', '', '</w>'): 'k,</w>',\n",
       "  ('o', 'ver'): 'over',\n",
       "  ('oo', 'l</w>'): 'ool</w>',\n",
       "  ('pre', 'tty</w>'): 'pretty</w>',\n",
       "  ('Zappo', 's</w>'): 'Zappos</w>',\n",
       "  ('t', 'y</w>'): 'ty</w>',\n",
       "  ('prob', 'le'): 'proble',\n",
       "  ('tong', 'ue</w>'): 'tongue</w>',\n",
       "  ('lar', '</w>'): 'lar</w>',\n",
       "  ('t', 's.</w>'): 'ts.</w>',\n",
       "  ('m', 'ely</w>'): 'mely</w>',\n",
       "  ('da', 'y.</w>'): 'day.</w>',\n",
       "  ('wh', 'ere</w>'): 'where</w>',\n",
       "  ('t', 'en'): 'ten',\n",
       "  ('the', 'se.</w>'): 'these.</w>',\n",
       "  ('the', \"y're</w>\"): \"they're</w>\",\n",
       "  ('m', 's</w>'): 'ms</w>',\n",
       "  ('revie', 'w'): 'review',\n",
       "  ('h', 'ard</w>'): 'hard</w>',\n",
       "  ('it', '.</w>'): 'it.</w>',\n",
       "  ('..', '.'): '...',\n",
       "  ('be', 'ing</w>'): 'being</w>',\n",
       "  ('rec', 'ei'): 'recei',\n",
       "  ('con', 'd</w>'): 'cond</w>',\n",
       "  ('gh', 't.</w>'): 'ght.</w>',\n",
       "  ('h', 'el'): 'hel',\n",
       "  ('th', 'at'): 'that',\n",
       "  ('wh', 'o</w>'): 'who</w>',\n",
       "  ('month', 's</w>'): 'months</w>',\n",
       "  ('p', 'e</w>'): 'pe</w>',\n",
       "  ('wom', 'en'): 'women',\n",
       "  ('sa', 'ppo'): 'sappo',\n",
       "  ('sappo', 'in'): 'sappoin',\n",
       "  ('u', 'sed</w>'): 'used</w>',\n",
       "  ('tre', 'mely</w>'): 'tremely</w>',\n",
       "  ('l', 'in'): 'lin',\n",
       "  ('soc', 'ks</w>'): 'socks</w>',\n",
       "  ('c', 'i'): 'ci',\n",
       "  ('l', 'es</w>'): 'les</w>',\n",
       "  ('s', '!</w>'): 's!</w>',\n",
       "  ('An', 'd</w>'): 'And</w>',\n",
       "  ('wid', 'th'): 'width',\n",
       "  ('st', 'a'): 'sta',\n",
       "  ('1/', '2</w>'): '1/2</w>',\n",
       "  ('d', 'uc'): 'duc',\n",
       "  ('well', '.</w>'): 'well.</w>',\n",
       "  ('a', 'v'): 'av',\n",
       "  ('ru', 'b'): 'rub',\n",
       "  ('u', 't'): 'ut',\n",
       "  ('m', 'm'): 'mm',\n",
       "  ('p', 'u'): 'pu',\n",
       "  ('com', 'e</w>'): 'come</w>',\n",
       "  ('to', 'p</w>'): 'top</w>',\n",
       "  ('foo', 't.</w>'): 'foot.</w>',\n",
       "  ('per', 'i'): 'peri',\n",
       "  ('I', 't'): 'It',\n",
       "  ('com', 'fort'): 'comfort',\n",
       "  ('T', '</w>'): 'T</w>',\n",
       "  ('b', 'le</w>'): 'ble</w>',\n",
       "  ('se', 'e</w>'): 'see</w>',\n",
       "  ('p', 'ri'): 'pri',\n",
       "  ('8', '</w>'): '8</w>',\n",
       "  ('a', 'maz'): 'amaz',\n",
       "  ('di', \"dn't</w>\"): \"didn't</w>\",\n",
       "  ('c', 'la'): 'cla',\n",
       "  ('perfec', 'tl'): 'perfectl',\n",
       "  ('thin', 'k</w>'): 'think</w>',\n",
       "  ('sin', 'ce</w>'): 'since</w>',\n",
       "  ('m', 'any</w>'): 'many</w>',\n",
       "  ('m', '.</w>'): 'm.</w>',\n",
       "  ('bo', 'th</w>'): 'both</w>',\n",
       "  ('me', 'di'): 'medi',\n",
       "  ('fit', 's</w>'): 'fits</w>',\n",
       "  ('b', 'est</w>'): 'best</w>',\n",
       "  ('or', 't</w>'): 'ort</w>',\n",
       "  ('f', 'i'): 'fi',\n",
       "  ('c', 'lo'): 'clo',\n",
       "  ('ic', 'k'): 'ick',\n",
       "  ('ti', 'ght</w>'): 'tight</w>',\n",
       "  ('tri', 'ed</w>'): 'tried</w>',\n",
       "  ('N', 'e'): 'Ne',\n",
       "  ('wid', 'er</w>'): 'wider</w>',\n",
       "  ('ab', 'ly</w>'): 'ably</w>',\n",
       "  ('The', \"y're</w>\"): \"They're</w>\",\n",
       "  ('di', 'sappoin'): 'disappoin',\n",
       "  ('at', 'ely</w>'): 'ately</w>',\n",
       "  ('f', 'ac'): 'fac',\n",
       "  ('p', 'l'): 'pl',\n",
       "  ('v', 'en'): 'ven',\n",
       "  ('be', 'for'): 'befor',\n",
       "  ('su', 'ally</w>'): 'sually</w>',\n",
       "  ('p', 'le</w>'): 'ple</w>',\n",
       "  ('on', '', '</w>'): 'on,</w>',\n",
       "  ('su', 're</w>'): 'sure</w>',\n",
       "  ('al', 'most</w>'): 'almost</w>',\n",
       "  ('L', 'i'): 'Li',\n",
       "  ('da', 'ys</w>'): 'days</w>',\n",
       "  ('f', 'ar</w>'): 'far</w>',\n",
       "  ('larg', 'er</w>'): 'larger</w>',\n",
       "  ('se', 'cond</w>'): 'second</w>',\n",
       "  ('tic', '</w>'): 'tic</w>',\n",
       "  ('P', 'er'): 'Per',\n",
       "  ('c', 're'): 'cre',\n",
       "  ('w', 'ar'): 'war',\n",
       "  ('cu', 'st'): 'cust',\n",
       "  ('7', '</w>'): '7</w>',\n",
       "  ('u', 'g'): 'ug',\n",
       "  ('4', '</w>'): '4</w>',\n",
       "  ('pec', 'i'): 'peci',\n",
       "  ('&', '</w>'): '&</w>',\n",
       "  ('9', '</w>'): '9</w>',\n",
       "  ('O', 'n'): 'On',\n",
       "  ('styli', 'sh'): 'stylish',\n",
       "  ('norm', 'ally</w>'): 'normally</w>',\n",
       "  ('es', 'n'): 'esn',\n",
       "  ('ell', 'ent</w>'): 'ellent</w>',\n",
       "  ('f', 'ound</w>'): 'found</w>',\n",
       "  ('ch', 'ang'): 'chang',\n",
       "  ('st', 'an'): 'stan',\n",
       "  ('o', 'le</w>'): 'ole</w>',\n",
       "  ('fort', 'un'): 'fortun',\n",
       "  ('1', '2</w>'): '12</w>',\n",
       "  ('sm', 'all</w>'): 'small</w>',\n",
       "  ('fec', 't</w>'): 'fect</w>',\n",
       "  ('materi', 'al</w>'): 'material</w>',\n",
       "  ('pair', '.</w>'): 'pair.</w>',\n",
       "  ('re', 'plac'): 'replac',\n",
       "  ('whi', 'le</w>'): 'while</w>',\n",
       "  ('the', 'n</w>'): 'then</w>',\n",
       "  ('ca', 'me</w>'): 'came</w>',\n",
       "  ('brea', 'k</w>'): 'break</w>',\n",
       "  ('c', '</w>'): 'c</w>',\n",
       "  ('ti', 'm'): 'tim',\n",
       "  ('br', 'an'): 'bran',\n",
       "  ('every', 'day</w>'): 'everyday</w>',\n",
       "  ('i', 'ly</w>'): 'ily</w>',\n",
       "  ('ac', 'tu'): 'actu',\n",
       "  ('re', 'd</w>'): 'red</w>',\n",
       "  ('de', 'finitely</w>'): 'definitely</w>',\n",
       "  ('comp', 'li'): 'compli',\n",
       "  ('compli', 'men'): 'complimen',\n",
       "  ('11', '</w>'): '11</w>',\n",
       "  ('fu', 'l</w>'): 'ful</w>',\n",
       "  ('j', 'o'): 'jo',\n",
       "  ('ver', 'si'): 'versi',\n",
       "  ('6', '</w>'): '6</w>',\n",
       "  ('pp', 'er</w>'): 'pper</w>',\n",
       "  ('hi', 'm</w>'): 'him</w>',\n",
       "  ('h', 'ea'): 'hea',\n",
       "  ('shoes', '!</w>'): 'shoes!</w>',\n",
       "  ('F', 'it</w>'): 'Fit</w>',\n",
       "  ('materi', 'al'): 'material',\n",
       "  (';', '</w>'): ';</w>',\n",
       "  ('it', 's</w>'): 'its</w>',\n",
       "  ('foo', 't'): 'foot',\n",
       "  ('l', 'ot</w>'): 'lot</w>',\n",
       "  ('wan', 'ted</w>'): 'wanted</w>',\n",
       "  ('p', 's</w>'): 'ps</w>',\n",
       "  ('bu', 'ying</w>'): 'buying</w>',\n",
       "  ('N', 'ice</w>'): 'Nice</w>',\n",
       "  ('ic', 'e.</w>'): 'ice.</w>',\n",
       "  ('u', 'm'): 'um',\n",
       "  ('d', 'es</w>'): 'des</w>',\n",
       "  ('sa', 'y</w>'): 'say</w>',\n",
       "  ('E', 'x'): 'Ex',\n",
       "  ('gh', 'ly</w>'): 'ghly</w>',\n",
       "  ('ab', 'le.</w>'): 'able.</w>',\n",
       "  ('do', 'esn'): 'doesn',\n",
       "  ('V', 'ans</w>'): 'Vans</w>',\n",
       "  ('c', 'ou'): 'cou',\n",
       "  ('thr', 'ough</w>'): 'through</w>',\n",
       "  ('U', 'n'): 'Un',\n",
       "  ('s', 'e', '</w>'): 'se,</w>',\n",
       "  ('w', 'e</w>'): 'we</w>',\n",
       "  ('pro', 'vi'): 'provi',\n",
       "  ('th', 'er.</w>'): 'ther.</w>',\n",
       "  ('ke', 'ep</w>'): 'keep</w>',\n",
       "  ('?', '</w>'): '?</w>',\n",
       "  ('gh', 't', '</w>'): 'ght,</w>',\n",
       "  ('D', '</w>'): 'D</w>',\n",
       "  ('p', '-'): 'p-',\n",
       "  ('sh', 'ould</w>'): 'should</w>',\n",
       "  ('d', 'ow'): 'dow',\n",
       "  ('v', 'or'): 'vor',\n",
       "  ('j', 'e'): 'je',\n",
       "  ('rea', 'dy</w>'): 'ready</w>',\n",
       "  ('ex', 'tremely</w>'): 'extremely</w>',\n",
       "  ('Su', 'per</w>'): 'Super</w>',\n",
       "  ('pa', 'd'): 'pad',\n",
       "  ('k', 's.</w>'): 'ks.</w>',\n",
       "  ('see', 'm</w>'): 'seem</w>',\n",
       "  ('ea', 'si'): 'easi',\n",
       "  ('er', '', '</w>'): 'er,</w>',\n",
       "  ('li', 'ght'): 'light',\n",
       "  ('w', 'on'): 'won',\n",
       "  ('Comfortab', 'le</w>'): 'Comfortable</w>',\n",
       "  ('st', 'ru'): 'stru',\n",
       "  ('v', 'e'): 've',\n",
       "  ('desig', 'n</w>'): 'design</w>',\n",
       "  ('b', 'er</w>'): 'ber</w>',\n",
       "  ('m', 'ar'): 'mar',\n",
       "  ('wan', 't</w>'): 'want</w>',\n",
       "  ('k', 'now</w>'): 'know</w>',\n",
       "  ('cushi', 'on'): 'cushion',\n",
       "  ('or', 'tho'): 'ortho',\n",
       "  ('P', 'ro'): 'Pro',\n",
       "  ('wei', 'ght</w>'): 'weight</w>',\n",
       "  ('th', 'ough</w>'): 'though</w>',\n",
       "  ('g', 'r'): 'gr',\n",
       "  ('norm', 'al</w>'): 'normal</w>',\n",
       "  ('pro', 'duc'): 'produc',\n",
       "  ('n', 'ow'): 'now',\n",
       "  ('u', 'sually</w>'): 'usually</w>',\n",
       "  ('tion', '.</w>'): 'tion.</w>',\n",
       "  ('small', 'er</w>'): 'smaller</w>',\n",
       "  ('A', 's</w>'): 'As</w>',\n",
       "  ('Per', 'fect</w>'): 'Perfect</w>',\n",
       "  ('t', 'in'): 'tin',\n",
       "  ('befor', 'e</w>'): 'before</w>',\n",
       "  ('Y', '</w>'): 'Y</w>',\n",
       "  ('di', 's'): 'dis',\n",
       "  ('oul', \"dn't</w>\"): \"ouldn't</w>\"},\n",
       " 'vocab': {'<PAD>': 0,\n",
       "  '<UNK>': 1,\n",
       "  'B': 2,\n",
       "  'o': 3,\n",
       "  'u': 4,\n",
       "  'g': 5,\n",
       "  'h': 6,\n",
       "  't': 7,\n",
       "  '</w>': 8,\n",
       "  'e': 9,\n",
       "  's': 10,\n",
       "  'f': 11,\n",
       "  'r': 12,\n",
       "  'm': 13,\n",
       "  'y': 14,\n",
       "  'n': 15,\n",
       "  'a': 16,\n",
       "  'd': 17,\n",
       "  'L': 18,\n",
       "  'O': 19,\n",
       "  'V': 20,\n",
       "  'E': 21,\n",
       "  'S': 22,\n",
       "  '!': 23,\n",
       "  'H': 24,\n",
       "  'w': 25,\n",
       "  'l': 26,\n",
       "  'v': 27,\n",
       "  'I': 28,\n",
       "  'i': 29,\n",
       "  'b': 30,\n",
       "  'p': 31,\n",
       "  'C': 32,\n",
       "  '.': 33,\n",
       "  'c': 34,\n",
       "  'D': 35,\n",
       "  \"'\": 36,\n",
       "  'P': 37,\n",
       "  'A': 38,\n",
       "  'z': 39,\n",
       "  'T': 40,\n",
       "  'k': 41,\n",
       "  'W': 42,\n",
       "  ',': 43,\n",
       "  '1': 44,\n",
       "  'j': 45,\n",
       "  'q': 46,\n",
       "  '…': 47,\n",
       "  '-': 48,\n",
       "  '&': 49,\n",
       "  'Z': 50,\n",
       "  '’': 51,\n",
       "  'J': 52,\n",
       "  'x': 53,\n",
       "  'G': 54,\n",
       "  '5': 55,\n",
       "  '0': 56,\n",
       "  '(': 57,\n",
       "  ')': 58,\n",
       "  ':': 59,\n",
       "  'F': 60,\n",
       "  '8': 61,\n",
       "  '+': 62,\n",
       "  '\"': 63,\n",
       "  'M': 64,\n",
       "  'N': 65,\n",
       "  'R': 66,\n",
       "  '9': 67,\n",
       "  '‘': 68,\n",
       "  '/': 69,\n",
       "  '2': 70,\n",
       "  '?': 71,\n",
       "  ';': 72,\n",
       "  '“': 73,\n",
       "  '”': 74,\n",
       "  '3': 75,\n",
       "  '4': 76,\n",
       "  'Y': 77,\n",
       "  '6': 78,\n",
       "  'Â': 79,\n",
       "  '½': 80,\n",
       "  'U': 81,\n",
       "  'K': 82,\n",
       "  '=': 83,\n",
       "  '\\\\': 84,\n",
       "  '7': 85,\n",
       "  'á': 86,\n",
       "  '$': 87,\n",
       "  'X': 88,\n",
       "  '%': 89,\n",
       "  '#': 90,\n",
       "  '*': 91,\n",
       "  'Q': 92,\n",
       "  '~': 93,\n",
       "  ']': 94,\n",
       "  '—': 95,\n",
       "  '>': 96,\n",
       "  '¹': 97,\n",
       "  '[': 98,\n",
       "  '`': 99,\n",
       "  '–': 100,\n",
       "  'e</w>': 101,\n",
       "  'th': 102,\n",
       "  't</w>': 103,\n",
       "  's</w>': 104,\n",
       "  'd</w>': 105,\n",
       "  '.</w>': 106,\n",
       "  'y</w>': 107,\n",
       "  'er': 108,\n",
       "  'an': 109,\n",
       "  'in': 110,\n",
       "  'or': 111,\n",
       "  'ar': 112,\n",
       "  'o</w>': 113,\n",
       "  'the</w>': 114,\n",
       "  'and</w>': 115,\n",
       "  'I</w>': 116,\n",
       "  'the': 117,\n",
       "  'sh': 118,\n",
       "  'on': 119,\n",
       "  'al': 120,\n",
       "  ',</w>': 121,\n",
       "  'ou': 122,\n",
       "  're': 123,\n",
       "  'for': 124,\n",
       "  'a</w>': 125,\n",
       "  'sho': 126,\n",
       "  'om': 127,\n",
       "  'ing': 128,\n",
       "  'en': 129,\n",
       "  'er</w>': 130,\n",
       "  'to</w>': 131,\n",
       "  'ing</w>': 132,\n",
       "  'es</w>': 133,\n",
       "  'is</w>': 134,\n",
       "  'ed</w>': 135,\n",
       "  'f</w>': 136,\n",
       "  'se</w>': 137,\n",
       "  'Th': 138,\n",
       "  'ha': 139,\n",
       "  'oo': 140,\n",
       "  've</w>': 141,\n",
       "  'wi': 142,\n",
       "  'st': 143,\n",
       "  'li': 144,\n",
       "  'ab': 145,\n",
       "  'l</w>': 146,\n",
       "  'le</w>': 147,\n",
       "  'ly</w>': 148,\n",
       "  'com': 149,\n",
       "  'at</w>': 150,\n",
       "  'si': 151,\n",
       "  'm</w>': 152,\n",
       "  'e.</w>': 153,\n",
       "  'we': 154,\n",
       "  'of</w>': 155,\n",
       "  'for</w>': 156,\n",
       "  'gh': 157,\n",
       "  'in</w>': 158,\n",
       "  'fe': 159,\n",
       "  'are</w>': 160,\n",
       "  'as</w>': 161,\n",
       "  'ti': 162,\n",
       "  'bu': 163,\n",
       "  'it</w>': 164,\n",
       "  'k</w>': 165,\n",
       "  'es': 166,\n",
       "  'on</w>': 167,\n",
       "  'fort': 168,\n",
       "  'ver': 169,\n",
       "  'it': 170,\n",
       "  'my</w>': 171,\n",
       "  'ch': 172,\n",
       "  'th</w>': 173,\n",
       "  'su': 174,\n",
       "  'shoes</w>': 175,\n",
       "  'they</w>': 176,\n",
       "  'ir': 177,\n",
       "  'fortab': 178,\n",
       "  'at': 179,\n",
       "  'ow': 180,\n",
       "  'ea': 181,\n",
       "  'comfortab': 182,\n",
       "  'these</w>': 183,\n",
       "  'st</w>': 184,\n",
       "  'ac': 185,\n",
       "  'but</w>': 186,\n",
       "  'an</w>': 187,\n",
       "  'The': 188,\n",
       "  'have</w>': 189,\n",
       "  '!</w>': 190,\n",
       "  'siz': 191,\n",
       "  'ic': 192,\n",
       "  'pp': 193,\n",
       "  'ot</w>': 194,\n",
       "  'wear': 195,\n",
       "  'ol': 196,\n",
       "  'pa': 197,\n",
       "  'with</w>': 198,\n",
       "  'them</w>': 199,\n",
       "  'loo': 200,\n",
       "  'be': 201,\n",
       "  'en</w>': 202,\n",
       "  'll</w>': 203,\n",
       "  'shoe</w>': 204,\n",
       "  'fee': 205,\n",
       "  'to': 206,\n",
       "  'n</w>': 207,\n",
       "  'ur': 208,\n",
       "  'that</w>': 209,\n",
       "  'so</w>': 210,\n",
       "  'The</w>': 211,\n",
       "  'e,</w>': 212,\n",
       "  'p</w>': 213,\n",
       "  'wh': 214,\n",
       "  'ke</w>': 215,\n",
       "  's.</w>': 216,\n",
       "  'ght</w>': 217,\n",
       "  'di': 218,\n",
       "  'was</w>': 219,\n",
       "  'very</w>': 220,\n",
       "  'gre': 221,\n",
       "  'pair': 222,\n",
       "  'ri': 223,\n",
       "  'comfortable</w>': 224,\n",
       "  't.</w>': 225,\n",
       "  'lo': 226,\n",
       "  'size</w>': 227,\n",
       "  'ally</w>': 228,\n",
       "  'you': 229,\n",
       "  'al</w>': 230,\n",
       "  'de</w>': 231,\n",
       "  'not</w>': 232,\n",
       "  'or</w>': 233,\n",
       "  'ch</w>': 234,\n",
       "  'tt': 235,\n",
       "  'se': 236,\n",
       "  'ne': 237,\n",
       "  'oul': 238,\n",
       "  'ere</w>': 239,\n",
       "  'out</w>': 240,\n",
       "  'wor': 241,\n",
       "  'ro': 242,\n",
       "  'this</w>': 243,\n",
       "  'me</w>': 244,\n",
       "  'ca': 245,\n",
       "  'wear</w>': 246,\n",
       "  'da': 247,\n",
       "  'ma': 248,\n",
       "  'be</w>': 249,\n",
       "  'fit</w>': 250,\n",
       "  'like</w>': 251,\n",
       "  'ther</w>': 252,\n",
       "  'ow</w>': 253,\n",
       "  'ex': 254,\n",
       "  'ru': 255,\n",
       "  'shoes': 256,\n",
       "  'so': 257,\n",
       "  'per': 258,\n",
       "  'de': 259,\n",
       "  \"'t</w>\": 260,\n",
       "  'They</w>': 261,\n",
       "  'ould</w>': 262,\n",
       "  \"I'\": 263,\n",
       "  'le': 264,\n",
       "  'king</w>': 265,\n",
       "  'sty': 266,\n",
       "  'd.</w>': 267,\n",
       "  'all</w>': 268,\n",
       "  'pair</w>': 269,\n",
       "  'y.</w>': 270,\n",
       "  'sa': 271,\n",
       "  'qu': 272,\n",
       "  'lac': 273,\n",
       "  'la': 274,\n",
       "  'These</w>': 275,\n",
       "  'men': 276,\n",
       "  'great</w>': 277,\n",
       "  'mo': 278,\n",
       "  'bo': 279,\n",
       "  'le.</w>': 280,\n",
       "  \"'s</w>\": 281,\n",
       "  'el': 282,\n",
       "  'sn': 283,\n",
       "  'un': 284,\n",
       "  'fec': 285,\n",
       "  'ge': 286,\n",
       "  'you</w>': 287,\n",
       "  'ood</w>': 288,\n",
       "  'der': 289,\n",
       "  'ks</w>': 290,\n",
       "  'ust</w>': 291,\n",
       "  'ore</w>': 292,\n",
       "  'wal': 293,\n",
       "  'too</w>': 294,\n",
       "  'wa': 295,\n",
       "  'than</w>': 296,\n",
       "  'om</w>': 297,\n",
       "  'look</w>': 298,\n",
       "  'ong': 299,\n",
       "  'were</w>': 300,\n",
       "  'ff': 301,\n",
       "  'them': 302,\n",
       "  'tr': 303,\n",
       "  'con': 304,\n",
       "  'supp': 305,\n",
       "  'hi': 306,\n",
       "  'w</w>': 307,\n",
       "  'gh</w>': 308,\n",
       "  'eak': 309,\n",
       "  'use</w>': 310,\n",
       "  'pe': 311,\n",
       "  'foo': 312,\n",
       "  'fr': 313,\n",
       "  'ear': 314,\n",
       "  'ely</w>': 315,\n",
       "  'all': 316,\n",
       "  'ver</w>': 317,\n",
       "  'ent</w>': 318,\n",
       "  'cu': 319,\n",
       "  'suppor': 320,\n",
       "  'rea': 321,\n",
       "  'other</w>': 322,\n",
       "  's,</w>': 323,\n",
       "  'had</w>': 324,\n",
       "  'fin': 325,\n",
       "  'col': 326,\n",
       "  'sli': 327,\n",
       "  'perfec': 328,\n",
       "  'just</w>': 329,\n",
       "  'sed</w>': 330,\n",
       "  'more</w>': 331,\n",
       "  'shoes.</w>': 332,\n",
       "  'feet</w>': 333,\n",
       "  'would</w>': 334,\n",
       "  'vi': 335,\n",
       "  'one</w>': 336,\n",
       "  'ought</w>': 337,\n",
       "  'love</w>': 338,\n",
       "  'sneak': 339,\n",
       "  'ice</w>': 340,\n",
       "  're</w>': 341,\n",
       "  'oun': 342,\n",
       "  '..': 343,\n",
       "  'ers</w>': 344,\n",
       "  'ery</w>': 345,\n",
       "  'wid': 346,\n",
       "  'good</w>': 347,\n",
       "  'lea': 348,\n",
       "  'som': 349,\n",
       "  't,</w>': 350,\n",
       "  'pro': 351,\n",
       "  'ke': 352,\n",
       "  'ter</w>': 353,\n",
       "  'sol': 354,\n",
       "  'wide</w>': 355,\n",
       "  'shoe.</w>': 356,\n",
       "  'lar': 357,\n",
       "  'up</w>': 358,\n",
       "  'Very</w>': 359,\n",
       "  'get</w>': 360,\n",
       "  'ce</w>': 361,\n",
       "  '5</w>': 362,\n",
       "  'Gre': 363,\n",
       "  'from</w>': 364,\n",
       "  'tur': 365,\n",
       "  'y,</w>': 366,\n",
       "  'Great</w>': 367,\n",
       "  'ing.</w>': 368,\n",
       "  'fit': 369,\n",
       "  'day</w>': 370,\n",
       "  'le,</w>': 371,\n",
       "  'order': 372,\n",
       "  'arr': 373,\n",
       "  'sm': 374,\n",
       "  'will</w>': 375,\n",
       "  'ag': 376,\n",
       "  'whi': 377,\n",
       "  'comfortable.</w>': 378,\n",
       "  'alit': 379,\n",
       "  \"I'm</w>\": 380,\n",
       "  'ho': 381,\n",
       "  '2</w>': 382,\n",
       "  'ts</w>': 383,\n",
       "  'them.</w>': 384,\n",
       "  'ig': 385,\n",
       "  'ys</w>': 386,\n",
       "  'most</w>': 387,\n",
       "  'My</w>': 388,\n",
       "  'er.</w>': 389,\n",
       "  'us': 390,\n",
       "  'really</w>': 391,\n",
       "  'can</w>': 392,\n",
       "  'cause</w>': 393,\n",
       "  'narr': 394,\n",
       "  'year': 395,\n",
       "  'qualit': 396,\n",
       "  'ough</w>': 397,\n",
       "  'This</w>': 398,\n",
       "  'ds</w>': 399,\n",
       "  'r</w>': 400,\n",
       "  'pre': 401,\n",
       "  'great': 402,\n",
       "  'ted</w>': 403,\n",
       "  'feel</w>': 404,\n",
       "  'mi': 405,\n",
       "  'sti': 406,\n",
       "  'am</w>': 407,\n",
       "  'comp': 408,\n",
       "  'ut</w>': 409,\n",
       "  'cha': 410,\n",
       "  'has</w>': 411,\n",
       "  'pur': 412,\n",
       "  'he': 413,\n",
       "  'looking</w>': 414,\n",
       "  'sh</w>': 415,\n",
       "  'long': 416,\n",
       "  'fter</w>': 417,\n",
       "  'ta': 418,\n",
       "  'when</w>': 419,\n",
       "  '10': 420,\n",
       "  'if</w>': 421,\n",
       "  'do': 422,\n",
       "  'fir': 423,\n",
       "  'litt': 424,\n",
       "  'about</w>': 425,\n",
       "  'mu': 426,\n",
       "  'because</w>': 427,\n",
       "  'ue</w>': 428,\n",
       "  'per</w>': 429,\n",
       "  'own</w>': 430,\n",
       "  'app': 431,\n",
       "  'don': 432,\n",
       "  'ite</w>': 433,\n",
       "  'pr': 434,\n",
       "  'ound</w>': 435,\n",
       "  'little</w>': 436,\n",
       "  'sole</w>': 437,\n",
       "  'bought</w>': 438,\n",
       "  'll': 439,\n",
       "  'purcha': 440,\n",
       "  'tion': 441,\n",
       "  'd,</w>': 442,\n",
       "  'te': 443,\n",
       "  'es.</w>': 444,\n",
       "  \"I've</w>\": 445,\n",
       "  'ordered</w>': 446,\n",
       "  'tly</w>': 447,\n",
       "  'no</w>': 448,\n",
       "  'sual</w>': 449,\n",
       "  'big': 450,\n",
       "  'br': 451,\n",
       "  'wearing</w>': 452,\n",
       "  'commen': 453,\n",
       "  'been</w>': 454,\n",
       "  'fu': 455,\n",
       "  'pec': 456,\n",
       "  'me': 457,\n",
       "  'der</w>': 458,\n",
       "  'fa': 459,\n",
       "  ')</w>': 460,\n",
       "  'ded</w>': 461,\n",
       "  'retur': 462,\n",
       "  'style</w>': 463,\n",
       "  'any</w>': 464,\n",
       "  '.5</w>': 465,\n",
       "  'Lo': 466,\n",
       "  'foot</w>': 467,\n",
       "  'able</w>': 468,\n",
       "  'which</w>': 469,\n",
       "  '-</w>': 470,\n",
       "  'box': 471,\n",
       "  'tic': 472,\n",
       "  'ev': 473,\n",
       "  \"'re</w>\": 474,\n",
       "  'dn': 475,\n",
       "  'well</w>': 476,\n",
       "  'ater': 477,\n",
       "  'some</w>': 478,\n",
       "  'tion</w>': 479,\n",
       "  'thing</w>': 480,\n",
       "  'do</w>': 481,\n",
       "  'he</w>': 482,\n",
       "  'po': 483,\n",
       "  'mon': 484,\n",
       "  'ever': 485,\n",
       "  'bac': 486,\n",
       "  'lt</w>': 487,\n",
       "  'support</w>': 488,\n",
       "  'arch</w>': 489,\n",
       "  'shi': 490,\n",
       "  'first</w>': 491,\n",
       "  'ra': 492,\n",
       "  'rec': 493,\n",
       "  'go': 494,\n",
       "  'k.</w>': 495,\n",
       "  'ding</w>': 496,\n",
       "  'recommen': 497,\n",
       "  'ked</w>': 498,\n",
       "  'ess</w>': 499,\n",
       "  'now</w>': 500,\n",
       "  'after</w>': 501,\n",
       "  'ved</w>': 502,\n",
       "  'op': 503,\n",
       "  'diff': 504,\n",
       "  'sy</w>': 505,\n",
       "  'ep': 506,\n",
       "  'tru': 507,\n",
       "  'casual</w>': 508,\n",
       "  'larg': 509,\n",
       "  'color': 510,\n",
       "  'bit</w>': 511,\n",
       "  'narrow</w>': 512,\n",
       "  'quality</w>': 513,\n",
       "  'se.</w>': 514,\n",
       "  'only</w>': 515,\n",
       "  'ate</w>': 516,\n",
       "  'toe</w>': 517,\n",
       "  'your</w>': 518,\n",
       "  'right</w>': 519,\n",
       "  'time</w>': 520,\n",
       "  'Love</w>': 521,\n",
       "  'slip</w>': 522,\n",
       "  'got</w>': 523,\n",
       "  'brea': 524,\n",
       "  \"don't</w>\": 525,\n",
       "  'hal': 526,\n",
       "  'ff</w>': 527,\n",
       "  'go</w>': 528,\n",
       "  'He</w>': 529,\n",
       "  'styli': 530,\n",
       "  'It</w>': 531,\n",
       "  'par': 532,\n",
       "  'no': 533,\n",
       "  'el</w>': 534,\n",
       "  'wha': 535,\n",
       "  'every': 536,\n",
       "  'bett': 537,\n",
       "  'orm': 538,\n",
       "  'ad': 539,\n",
       "  'buy</w>': 540,\n",
       "  'much</w>': 541,\n",
       "  '11': 542,\n",
       "  'still</w>': 543,\n",
       "  'off': 544,\n",
       "  'soc': 545,\n",
       "  'sk': 546,\n",
       "  \"y're</w>\": 547,\n",
       "  'small': 548,\n",
       "  'work</w>': 549,\n",
       "  'worn</w>': 550,\n",
       "  'uc': 551,\n",
       "  'way</w>': 552,\n",
       "  'usb': 553,\n",
       "  'est</w>': 554,\n",
       "  'walking</w>': 555,\n",
       "  'I’': 556,\n",
       "  '!!</w>': 557,\n",
       "  'long</w>': 558,\n",
       "  'half</w>': 559,\n",
       "  'ar</w>': 560,\n",
       "  'perfect</w>': 561,\n",
       "  '’t</w>': 562,\n",
       "  'size.</w>': 563,\n",
       "  'nice</w>': 564,\n",
       "  'norm': 565,\n",
       "  'ard</w>': 566,\n",
       "  'even</w>': 567,\n",
       "  'ans</w>': 568,\n",
       "  'me.</w>': 569,\n",
       "  'feel': 570,\n",
       "  'back</w>': 571,\n",
       "  'ic</w>': 572,\n",
       "  'true</w>': 573,\n",
       "  'differ': 574,\n",
       "  'gi': 575,\n",
       "  'ei': 576,\n",
       "  'ateri': 577,\n",
       "  'wan': 578,\n",
       "  'comfortable,</w>': 579,\n",
       "  'husb': 580,\n",
       "  'color</w>': 581,\n",
       "  'see': 582,\n",
       "  'what</w>': 583,\n",
       "  'blac': 584,\n",
       "  '\"</w>': 585,\n",
       "  'put</w>': 586,\n",
       "  \"dn't</w>\": 587,\n",
       "  'sig': 588,\n",
       "  'also</w>': 589,\n",
       "  '’s</w>': 590,\n",
       "  'leather</w>': 591,\n",
       "  'ying</w>': 592,\n",
       "  'great.</w>': 593,\n",
       "  'around</w>': 594,\n",
       "  'kes</w>': 595,\n",
       "  'Com': 596,\n",
       "  'another</w>': 597,\n",
       "  'tri': 598,\n",
       "  'fort</w>': 599,\n",
       "  'old</w>': 600,\n",
       "  'materi': 601,\n",
       "  'appo': 602,\n",
       "  'happ': 603,\n",
       "  'im': 604,\n",
       "  'can': 605,\n",
       "  'in.</w>': 606,\n",
       "  'with': 607,\n",
       "  'ell': 608,\n",
       "  'tu': 609,\n",
       "  'how</w>': 610,\n",
       "  'and': 611,\n",
       "  'ther': 612,\n",
       "  'Zappo': 613,\n",
       "  'ves</w>': 614,\n",
       "  'super</w>': 615,\n",
       "  'felt</w>': 616,\n",
       "  'lu': 617,\n",
       "  'ter': 618,\n",
       "  'well': 619,\n",
       "  'side</w>': 620,\n",
       "  'new</w>': 621,\n",
       "  'ft</w>': 622,\n",
       "  '3</w>': 623,\n",
       "  'ned</w>': 624,\n",
       "  'exc': 625,\n",
       "  'shoes,</w>': 626,\n",
       "  'on.</w>': 627,\n",
       "  'too': 628,\n",
       "  'ssi': 629,\n",
       "  'ss</w>': 630,\n",
       "  'te</w>': 631,\n",
       "  'ever</w>': 632,\n",
       "  'wee': 633,\n",
       "  'soles</w>': 634,\n",
       "  'sneakers</w>': 635,\n",
       "  'his</w>': 636,\n",
       "  'ang': 637,\n",
       "  'sin': 638,\n",
       "  'feet.</w>': 639,\n",
       "  'ser': 640,\n",
       "  'shoe,</w>': 641,\n",
       "  'easy</w>': 642,\n",
       "  'is': 643,\n",
       "  'pairs</w>': 644,\n",
       "  'ght': 645,\n",
       "  'better</w>': 646,\n",
       "  'recommend</w>': 647,\n",
       "  'any': 648,\n",
       "  'made</w>': 649,\n",
       "  'different</w>': 650,\n",
       "  'comfort</w>': 651,\n",
       "  'same</w>': 652,\n",
       "  'gu': 653,\n",
       "  'i</w>': 654,\n",
       "  'aga': 655,\n",
       "  'husband</w>': 656,\n",
       "  'box</w>': 657,\n",
       "  'prob': 658,\n",
       "  'month': 659,\n",
       "  'wom': 660,\n",
       "  'expec': 661,\n",
       "  'ir</w>': 662,\n",
       "  'tre': 663,\n",
       "  'walk</w>': 664,\n",
       "  'vie': 665,\n",
       "  'E</w>': 666,\n",
       "  'il': 667,\n",
       "  'tho': 668,\n",
       "  'desig': 669,\n",
       "  'ways</w>': 670,\n",
       "  'width</w>': 671,\n",
       "  'run</w>': 672,\n",
       "  'hou': 673,\n",
       "  'laces</w>': 674,\n",
       "  'nee': 675,\n",
       "  'up': 676,\n",
       "  'by</w>': 677,\n",
       "  'dy</w>': 678,\n",
       "  'order</w>': 679,\n",
       "  \"it's</w>\": 680,\n",
       "  'last</w>': 681,\n",
       "  'ty': 682,\n",
       "  'So</w>': 683,\n",
       "  'S</w>': 684,\n",
       "  'black</w>': 685,\n",
       "  'need</w>': 686,\n",
       "  'n.</w>': 687,\n",
       "  'ba': 688,\n",
       "  'out': 689,\n",
       "  'could</w>': 690,\n",
       "  'some': 691,\n",
       "  'pic': 692,\n",
       "  'high</w>': 693,\n",
       "  'revie': 694,\n",
       "  'wei': 695,\n",
       "  'sp': 696,\n",
       "  'end</w>': 697,\n",
       "  'down</w>': 698,\n",
       "  'light</w>': 699,\n",
       "  'find</w>': 700,\n",
       "  'ep</w>': 701,\n",
       "  'there</w>': 702,\n",
       "  'looks</w>': 703,\n",
       "  'Re': 704,\n",
       "  'over</w>': 705,\n",
       "  'heel</w>': 706,\n",
       "  'Ske': 707,\n",
       "  'dre': 708,\n",
       "  'ving</w>': 709,\n",
       "  'If</w>': 710,\n",
       "  'Wi': 711,\n",
       "  'tl': 712,\n",
       "  'star': 713,\n",
       "  ').</w>': 714,\n",
       "  'make</w>': 715,\n",
       "  'tong': 716,\n",
       "  'cushi': 717,\n",
       "  'goo': 718,\n",
       "  'wore</w>': 719,\n",
       "  'enough</w>': 720,\n",
       "  'son</w>': 721,\n",
       "  'es,</w>': 722,\n",
       "  '1/': 723,\n",
       "  'off</w>': 724,\n",
       "  'purchased</w>': 725,\n",
       "  'ank': 726,\n",
       "  'wo</w>': 727,\n",
       "  'thin': 728,\n",
       "  'plac': 729,\n",
       "  'without</w>': 730,\n",
       "  'return</w>': 731,\n",
       "  'ough': 732,\n",
       "  'Su': 733,\n",
       "  'run': 734,\n",
       "  'ing,</w>': 735,\n",
       "  'did</w>': 736,\n",
       "  'always</w>': 737,\n",
       "  'Al': 738,\n",
       "  'Sho': 739,\n",
       "  'sur': 740,\n",
       "  'finit': 741,\n",
       "  'finitely</w>': 742,\n",
       "  '...</w>': 743,\n",
       "  'walk': 744,\n",
       "  'few</w>': 745,\n",
       "  'years</w>': 746,\n",
       "  'support.</w>': 747,\n",
       "  'shoe': 748,\n",
       "  'fl': 749,\n",
       "  'ning</w>': 750,\n",
       "  'An': 751,\n",
       "  'maz': 752,\n",
       "  'ed.</w>': 753,\n",
       "  'Good</w>': 754,\n",
       "  'Not</w>': 755,\n",
       "  'white</w>': 756,\n",
       "  'Ha': 757,\n",
       "  'big</w>': 758,\n",
       "  'thr': 759,\n",
       "  'fit.</w>': 760,\n",
       "  'try</w>': 761,\n",
       "  '0</w>': 762,\n",
       "  'every</w>': 763,\n",
       "  'ous</w>': 764,\n",
       "  'sneaker': 765,\n",
       "  'dur': 766,\n",
       "  'A</w>': 767,\n",
       "  'less</w>': 768,\n",
       "  'But</w>': 769,\n",
       "  'last': 770,\n",
       "  'tting</w>': 771,\n",
       "  'loves</w>': 772,\n",
       "  'tty</w>': 773,\n",
       "  'stylish</w>': 774,\n",
       "  'I’m</w>': 775,\n",
       "  'going</w>': 776,\n",
       "  'happy</w>': 777,\n",
       "  'into</w>': 778,\n",
       "  '10</w>': 779,\n",
       "  'Comfortab': 780,\n",
       "  'their</w>': 781,\n",
       "  'sneaker</w>': 782,\n",
       "  'two</w>': 783,\n",
       "  'chers</w>': 784,\n",
       "  'oc': 785,\n",
       "  '.5': 786,\n",
       "  'comf': 787,\n",
       "  'min': 788,\n",
       "  'never</w>': 789,\n",
       "  'ni': 790,\n",
       "  'id</w>': 791,\n",
       "  'ance</w>': 792,\n",
       "  'narrow': 793,\n",
       "  'Ho': 794,\n",
       "  'ye': 795,\n",
       "  'wear.</w>': 796,\n",
       "  'issu': 797,\n",
       "  'feels</w>': 798,\n",
       "  'ting</w>': 799,\n",
       "  'ppo': 800,\n",
       "  'tive</w>': 801,\n",
       "  'k,</w>': 802,\n",
       "  'over': 803,\n",
       "  'ool</w>': 804,\n",
       "  'pretty</w>': 805,\n",
       "  'Zappos</w>': 806,\n",
       "  'ty</w>': 807,\n",
       "  'proble': 808,\n",
       "  'tongue</w>': 809,\n",
       "  'lar</w>': 810,\n",
       "  'ts.</w>': 811,\n",
       "  'mely</w>': 812,\n",
       "  'day.</w>': 813,\n",
       "  'where</w>': 814,\n",
       "  'ten': 815,\n",
       "  'these.</w>': 816,\n",
       "  \"they're</w>\": 817,\n",
       "  'ms</w>': 818,\n",
       "  'review': 819,\n",
       "  'hard</w>': 820,\n",
       "  'it.</w>': 821,\n",
       "  '...': 822,\n",
       "  'being</w>': 823,\n",
       "  'recei': 824,\n",
       "  'cond</w>': 825,\n",
       "  'ght.</w>': 826,\n",
       "  'hel': 827,\n",
       "  'that': 828,\n",
       "  'who</w>': 829,\n",
       "  'months</w>': 830,\n",
       "  'pe</w>': 831,\n",
       "  'women': 832,\n",
       "  'sappo': 833,\n",
       "  'sappoin': 834,\n",
       "  'used</w>': 835,\n",
       "  'tremely</w>': 836,\n",
       "  'lin': 837,\n",
       "  'socks</w>': 838,\n",
       "  'ci': 839,\n",
       "  'les</w>': 840,\n",
       "  's!</w>': 841,\n",
       "  'And</w>': 842,\n",
       "  'width': 843,\n",
       "  'sta': 844,\n",
       "  '1/2</w>': 845,\n",
       "  'duc': 846,\n",
       "  'well.</w>': 847,\n",
       "  'av': 848,\n",
       "  'rub': 849,\n",
       "  'ut': 850,\n",
       "  'mm': 851,\n",
       "  'pu': 852,\n",
       "  'come</w>': 853,\n",
       "  'top</w>': 854,\n",
       "  'foot.</w>': 855,\n",
       "  'peri': 856,\n",
       "  'It': 857,\n",
       "  'comfort': 858,\n",
       "  'T</w>': 859,\n",
       "  'ble</w>': 860,\n",
       "  'see</w>': 861,\n",
       "  'pri': 862,\n",
       "  '8</w>': 863,\n",
       "  'amaz': 864,\n",
       "  \"didn't</w>\": 865,\n",
       "  'cla': 866,\n",
       "  'perfectl': 867,\n",
       "  'think</w>': 868,\n",
       "  'since</w>': 869,\n",
       "  'many</w>': 870,\n",
       "  'm.</w>': 871,\n",
       "  'both</w>': 872,\n",
       "  'medi': 873,\n",
       "  'fits</w>': 874,\n",
       "  'best</w>': 875,\n",
       "  'ort</w>': 876,\n",
       "  'fi': 877,\n",
       "  'clo': 878,\n",
       "  'ick': 879,\n",
       "  'tight</w>': 880,\n",
       "  'tried</w>': 881,\n",
       "  'Ne': 882,\n",
       "  'wider</w>': 883,\n",
       "  'ably</w>': 884,\n",
       "  \"They're</w>\": 885,\n",
       "  'disappoin': 886,\n",
       "  'ately</w>': 887,\n",
       "  'fac': 888,\n",
       "  'pl': 889,\n",
       "  'ven': 890,\n",
       "  'befor': 891,\n",
       "  'sually</w>': 892,\n",
       "  'ple</w>': 893,\n",
       "  'on,</w>': 894,\n",
       "  'sure</w>': 895,\n",
       "  'almost</w>': 896,\n",
       "  'Li': 897,\n",
       "  'days</w>': 898,\n",
       "  'far</w>': 899,\n",
       "  'larger</w>': 900,\n",
       "  'second</w>': 901,\n",
       "  'tic</w>': 902,\n",
       "  'Per': 903,\n",
       "  'cre': 904,\n",
       "  'war': 905,\n",
       "  'cust': 906,\n",
       "  '7</w>': 907,\n",
       "  'ug': 908,\n",
       "  '4</w>': 909,\n",
       "  'peci': 910,\n",
       "  '&</w>': 911,\n",
       "  '9</w>': 912,\n",
       "  'On': 913,\n",
       "  'stylish': 914,\n",
       "  'normally</w>': 915,\n",
       "  'esn': 916,\n",
       "  'ellent</w>': 917,\n",
       "  'found</w>': 918,\n",
       "  'chang': 919,\n",
       "  'stan': 920,\n",
       "  'ole</w>': 921,\n",
       "  'fortun': 922,\n",
       "  '12</w>': 923,\n",
       "  'small</w>': 924,\n",
       "  'fect</w>': 925,\n",
       "  'material</w>': 926,\n",
       "  'pair.</w>': 927,\n",
       "  'replac': 928,\n",
       "  'while</w>': 929,\n",
       "  'then</w>': 930,\n",
       "  'came</w>': 931,\n",
       "  'break</w>': 932,\n",
       "  'c</w>': 933,\n",
       "  'tim': 934,\n",
       "  'bran': 935,\n",
       "  'everyday</w>': 936,\n",
       "  'ily</w>': 937,\n",
       "  'actu': 938,\n",
       "  'red</w>': 939,\n",
       "  'definitely</w>': 940,\n",
       "  'compli': 941,\n",
       "  'complimen': 942,\n",
       "  '11</w>': 943,\n",
       "  'ful</w>': 944,\n",
       "  'jo': 945,\n",
       "  'versi': 946,\n",
       "  '6</w>': 947,\n",
       "  'pper</w>': 948,\n",
       "  'him</w>': 949,\n",
       "  'hea': 950,\n",
       "  'shoes!</w>': 951,\n",
       "  'Fit</w>': 952,\n",
       "  'material': 953,\n",
       "  ';</w>': 954,\n",
       "  'its</w>': 955,\n",
       "  'foot': 956,\n",
       "  'lot</w>': 957,\n",
       "  'wanted</w>': 958,\n",
       "  'ps</w>': 959,\n",
       "  'buying</w>': 960,\n",
       "  'Nice</w>': 961,\n",
       "  'ice.</w>': 962,\n",
       "  'um': 963,\n",
       "  'des</w>': 964,\n",
       "  'say</w>': 965,\n",
       "  'Ex': 966,\n",
       "  'ghly</w>': 967,\n",
       "  'able.</w>': 968,\n",
       "  'doesn': 969,\n",
       "  'Vans</w>': 970,\n",
       "  'cou': 971,\n",
       "  'through</w>': 972,\n",
       "  'Un': 973,\n",
       "  'se,</w>': 974,\n",
       "  'we</w>': 975,\n",
       "  'provi': 976,\n",
       "  'ther.</w>': 977,\n",
       "  'keep</w>': 978,\n",
       "  '?</w>': 979,\n",
       "  'ght,</w>': 980,\n",
       "  'D</w>': 981,\n",
       "  'p-': 982,\n",
       "  'should</w>': 983,\n",
       "  'dow': 984,\n",
       "  'vor': 985,\n",
       "  'je': 986,\n",
       "  'ready</w>': 987,\n",
       "  'extremely</w>': 988,\n",
       "  'Super</w>': 989,\n",
       "  'pad': 990,\n",
       "  'ks.</w>': 991,\n",
       "  'seem</w>': 992,\n",
       "  'easi': 993,\n",
       "  'er,</w>': 994,\n",
       "  'light': 995,\n",
       "  'won': 996,\n",
       "  'Comfortable</w>': 997,\n",
       "  'stru': 998,\n",
       "  've': 999,\n",
       "  ...},\n",
       " 'stop_word': '</w>'}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d2395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
